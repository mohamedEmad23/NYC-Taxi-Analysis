{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6e6ec5",
   "metadata": {},
   "source": [
    "#### SparkML Task: Trip Profiling: Predict Likelihood of High Tipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb1266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configure Spark environment\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'\n",
    "\n",
    "# Create a Spark session with improved configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"NYC Taxi Model\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"20\") \\\n",
    "    .config(\"spark.default.parallelism\", \"20\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark session\n",
    "print(\"Spark session created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbe6053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 20:55:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+------------------+--------------------+--------------+-------------------+--------------------+---------------+----------------+-------------+------------------+------------------+-----------+----------+------------------+------------------+------------------+-----------+----------+------------+-----------+--------+------------+------------+---------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|pickup_location_id|    pickup_zone_name|pickup_borough|dropoff_location_id|   dropoff_zone_name|dropoff_borough|is_inter_borough|trip_distance|trip_duration_mins|     avg_speed_mph|fare_amount|tip_amount|    tip_percentage|        total_cost|     cost_per_mile|pickup_hour|pickup_day|pickup_month|pickup_year|day_type| time_of_day|payment_type|rate_code|\n",
      "+---------+-------------------+-------------------+---------------+------------------+--------------------+--------------+-------------------+--------------------+---------------+----------------+-------------+------------------+------------------+-----------+----------+------------------+------------------+------------------+-----------+----------+------------+-----------+--------+------------+------------+---------+\n",
      "|        1|2018-06-13 21:38:00|2018-06-13 22:01:00|            1.0|               229|Sutton Place/Turt...|     Manhattan|                244|Washington Height...|      Manhattan|           false|          8.2|              23.0|21.391304347826082|       26.0|      5.45|20.961538461538463|             32.75|3.1707317073170733|         21|         4|           6|       2018| weekday|       night|           1|        1|\n",
      "|        1|2018-12-01 11:40:00|2018-12-01 12:00:00|            1.0|                79|        East Village|     Manhattan|                231|TriBeCa/Civic Center|      Manhattan|           false|          2.0|              20.0|               6.0|       13.5|      2.15|15.925925925925924|             16.45|              6.75|         11|         7|          12|       2018| weekend|      midday|           1|        1|\n",
      "|        1|2018-03-09 20:51:00|2018-03-09 21:06:00|            2.0|               161|      Midtown Center|     Manhattan|                249|        West Village|      Manhattan|           false|          1.9|              15.0|               7.6|       11.0|      2.45|22.272727272727273|             14.75|5.7894736842105265|         20|         6|           3|       2018| weekday|       night|           1|        1|\n",
      "|        1|2018-02-21 18:26:00|2018-02-21 18:44:00|            1.0|               233| UN/Turtle Bay South|     Manhattan|                262|      Yorkville East|      Manhattan|           false|          2.6|              18.0| 8.666666666666668|       13.0|       0.0|               0.0|              14.8|               5.0|         18|         4|           2|       2018| weekday|evening_rush|           2|        1|\n",
      "|        1|2018-08-12 22:02:00|2018-08-12 22:13:00|            2.0|               162|        Midtown East|     Manhattan|                140|     Lenox Hill East|      Manhattan|           false|          2.4|              11.0|13.090909090909092|        9.5|       2.7|28.421052631578945|              13.5|3.9583333333333335|         22|         1|           8|       2018| weekend|       night|           1|        1|\n",
      "|        1|2018-07-25 09:23:00|2018-07-25 09:27:00|            1.0|               125|           Hudson Sq|     Manhattan|                144| Little Italy/NoLiTa|      Manhattan|           false|          0.5|               4.0|               7.5|        4.5|      1.05|23.333333333333332|              6.35|               9.0|          9|         4|           7|       2018| weekday|morning_rush|           1|        1|\n",
      "|        1|2018-03-08 08:52:00|2018-03-08 09:11:00|            1.0|               236|Upper East Side N...|     Manhattan|                 88|Financial Distric...|      Manhattan|           false|          7.4|              19.0|23.368421052631582|       22.5|      4.65|20.666666666666668|             27.95|3.0405405405405403|          8|         5|           3|       2018| weekday|morning_rush|           1|        1|\n",
      "|        1|2018-12-18 22:10:00|2018-12-18 22:25:00|            1.0|                13|   Battery Park City|     Manhattan|                233| UN/Turtle Bay South|      Manhattan|           false|          6.1|              15.0|              24.4|       19.5|       1.0| 5.128205128205128|              21.8|  3.19672131147541|         22|         3|          12|       2018| weekday|       night|           1|        1|\n",
      "|        1|2018-03-09 22:39:00|2018-03-09 23:03:00|            1.0|                79|        East Village|     Manhattan|                141|     Lenox Hill West|      Manhattan|           false|          3.6|              24.0|               9.0|       17.5|       1.0| 5.714285714285714|              19.8| 4.861111111111111|         22|         6|           3|       2018| weekday|       night|           1|        1|\n",
      "|        2|2018-03-08 09:15:00|2018-03-08 09:25:00|            1.0|               186|Penn Station/Madi...|     Manhattan|                230|Times Sq/Theatre ...|      Manhattan|           false|         0.85|              10.0|5.1000000000000005|        7.5|      1.24| 16.53333333333333| 9.540000000000001| 8.823529411764707|          9|         5|           3|       2018| weekday|morning_rush|           1|        1|\n",
      "|        1|2018-01-17 08:45:00|2018-01-17 08:55:00|            1.0|                75|   East Harlem South|     Manhattan|                142| Lincoln Square East|      Manhattan|           false|          2.3|              10.0|13.799999999999999|        9.5|       1.0|10.526315789473683|              11.3| 4.130434782608696|          8|         4|           1|       2018| weekday|morning_rush|           1|        1|\n",
      "|        2|2018-08-12 15:10:00|2018-08-12 15:18:00|            1.0|                24|        Bloomingdale|     Manhattan|                239|Upper West Side S...|      Manhattan|           false|         1.65|               8.0|            12.375|        8.0|       0.0|               0.0|               8.8| 4.848484848484849|         15|         1|           8|       2018| weekend|      midday|           2|        1|\n",
      "|        1|2018-02-03 16:31:00|2018-02-03 16:38:00|            1.0|               142| Lincoln Square East|     Manhattan|                230|Times Sq/Theatre ...|      Manhattan|           false|          0.7|               7.0| 5.999999999999999|        6.5|       0.0|               0.0|               7.3| 9.285714285714286|         16|         7|           2|       2018| weekend|evening_rush|           2|        1|\n",
      "|        1|2018-01-03 21:40:00|2018-01-03 21:54:00|            1.0|               114|Greenwich Village...|     Manhattan|                 66|  DUMBO/Vinegar Hill|       Brooklyn|            true|          2.8|              14.0|11.999999999999998|       12.5|      2.75|              22.0|             16.55| 4.464285714285714|         21|         4|           1|       2018| weekday|       night|           1|        1|\n",
      "|        1|2018-08-21 08:35:00|2018-08-21 08:40:00|            1.0|               162|        Midtown East|     Manhattan|                163|       Midtown North|      Manhattan|           false|          0.5|               5.0|               6.0|        5.0|      0.75|              15.0|              6.55|              10.0|          8|         3|           8|       2018| weekday|morning_rush|           1|        1|\n",
      "|        2|2018-06-06 22:07:00|2018-06-06 22:39:00|            1.0|               162|        Midtown East|     Manhattan|                 49|        Clinton Hill|       Brooklyn|            true|         8.91|              32.0|          16.70625|       30.0|       4.0|13.333333333333334|              35.3|3.3670033670033668|         22|         4|           6|       2018| weekday|       night|           1|        1|\n",
      "|        2|2018-11-16 01:19:00|2018-11-16 01:55:00|            2.0|               132|         JFK Airport|        Queens|                237|Upper East Side S...|      Manhattan|            true|        20.66|              36.0| 34.43333333333334|       52.0|       0.0|               0.0|58.559999999999995|2.5169409486931267|          1|         6|          11|       2018| weekday|       night|           2|        2|\n",
      "|        1|2018-01-09 16:12:00|2018-01-09 16:19:00|            1.0|               141|     Lenox Hill West|     Manhattan|                237|Upper East Side S...|      Manhattan|           false|          0.5|               7.0| 4.285714285714286|        5.5|       0.0|               0.0|               7.3|              11.0|         16|         3|           1|       2018| weekday|evening_rush|           2|        1|\n",
      "|        1|2018-02-06 20:08:00|2018-02-06 20:15:00|            1.0|               161|      Midtown Center|     Manhattan|                164|       Midtown South|      Manhattan|           false|          1.1|               7.0| 9.428571428571429|        6.5|      1.95|              30.0|              9.75| 5.909090909090908|         20|         3|           2|       2018| weekday|       night|           1|        1|\n",
      "|        2|2018-01-27 21:40:00|2018-01-27 21:56:00|            3.0|               237|Upper East Side S...|     Manhattan|                113|Greenwich Village...|      Manhattan|           false|         2.64|              16.0|               9.9|       12.0|       0.0|               0.0|              13.3| 4.545454545454545|         21|         7|           1|       2018| weekend|       night|           2|        1|\n",
      "+---------+-------------------+-------------------+---------------+------------------+--------------------+--------------+-------------------+--------------------+---------------+----------------+-------------+------------------+------------------+-----------+----------+------------------+------------------+------------------+-----------+----------+------------+-----------+--------+------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_data = spark.read.parquet(\"/root/DevDataOps/nyc-taxi-analysis/processed-data/nyc_taxi_processed.parquet\")\n",
    "taxi_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b9879",
   "metadata": {},
   "source": [
    "## Setting Up Models For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd0cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "root\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- pickup_zone_name: string (nullable = true)\n",
      " |-- pickup_borough: string (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- dropoff_zone_name: string (nullable = true)\n",
      " |-- dropoff_borough: string (nullable = true)\n",
      " |-- is_inter_borough: boolean (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- trip_duration_mins: double (nullable = true)\n",
      " |-- avg_speed_mph: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tip_percentage: double (nullable = true)\n",
      " |-- total_cost: double (nullable = true)\n",
      " |-- cost_per_mile: double (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_day: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- day_type: string (nullable = true)\n",
      " |-- time_of_day: string (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Copy Parquet File to not tamper with the original one\n",
    "taxi_data_copy = taxi_data\n",
    "\n",
    "# taxi_data_copy.show()\n",
    "num_columns = len(taxi_data.columns)\n",
    "print(num_columns)\n",
    "\n",
    "\n",
    "taxi_data_copy.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79367d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+---------------+-------------+------------------+-----------+------------------+------------------+--------------+---------------+----------+--------+\n",
      "|pickup_hour|pickup_day_of_week|passenger_count|trip_distance|trip_duration_mins|fare_amount|     fare_per_mile|   fare_per_minute|pickup_borough|dropoff_borough|tip_amount|high_tip|\n",
      "+-----------+------------------+---------------+-------------+------------------+-----------+------------------+------------------+--------------+---------------+----------+--------+\n",
      "|         21|                 4|            1.0|          8.2|              23.0|       26.0|3.1707317073170733|1.1304347826086956|     Manhattan|      Manhattan|      5.45|       1|\n",
      "|         11|                 7|            1.0|          2.0|              20.0|       13.5|              6.75|             0.675|     Manhattan|      Manhattan|      2.15|       1|\n",
      "|         20|                 6|            2.0|          1.9|              15.0|       11.0|5.7894736842105265|0.7333333333333333|     Manhattan|      Manhattan|      2.45|       1|\n",
      "|         18|                 4|            1.0|          2.6|              18.0|       13.0|               5.0|0.7222222222222222|     Manhattan|      Manhattan|       0.0|       0|\n",
      "|         22|                 1|            2.0|          2.4|              11.0|        9.5|3.9583333333333335|0.8636363636363636|     Manhattan|      Manhattan|       2.7|       1|\n",
      "+-----------+------------------+---------------+-------------+------------------+-----------+------------------+------------------+--------------+---------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, hour, dayofweek, expr\n",
    "\n",
    "# Create binary target column for high_tip (1 if tip_amount > 0.15 * fare_amount, 0 otherwise)\n",
    "taxi_data_ml = taxi_data_copy.withColumn(\n",
    "    \"high_tip\", \n",
    "    when(col(\"tip_amount\") > (0.15 * col(\"fare_amount\")), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Feature engineering\n",
    "# 1. Extract time features\n",
    "taxi_data_ml = taxi_data_ml.withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\")))\n",
    "taxi_data_ml = taxi_data_ml.withColumn(\"pickup_day_of_week\", dayofweek(col(\"pickup_datetime\")))\n",
    "\n",
    "# 2. Create fare_per_mile feature\n",
    "taxi_data_ml = taxi_data_ml.withColumn(\n",
    "    \"fare_per_mile\",\n",
    "    when(col(\"trip_distance\") > 0, col(\"fare_amount\") / col(\"trip_distance\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# 3. Calculate fare_per_minute feature\n",
    "taxi_data_ml = taxi_data_ml.withColumn(\n",
    "    \"fare_per_minute\",\n",
    "    when(col(\"trip_duration_mins\") > 0, col(\"fare_amount\") / col(\"trip_duration_mins\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# Display the first few rows with the new features\n",
    "taxi_data_ml.select(\n",
    "    \"pickup_hour\", \"pickup_day_of_week\", \"passenger_count\", \"trip_distance\", \n",
    "    \"trip_duration_mins\", \"fare_amount\", \"fare_per_mile\", \"fare_per_minute\",\n",
    "    \"pickup_borough\", \"dropoff_borough\", \"tip_amount\", \"high_tip\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2664cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------------------+---------------+---------------------+-------------------+\n",
      "|pickup_borough|pickup_borough_index|pickup_borough_vec|dropoff_borough|dropoff_borough_index|dropoff_borough_vec|\n",
      "+--------------+--------------------+------------------+---------------+---------------------+-------------------+\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "+--------------+--------------------+------------------+---------------+---------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Handle categorical variables: pickup_borough and dropoff_borough\n",
    "# Step 1: Convert string columns to indices\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
    "    for col_name in [\"pickup_borough\", \"dropoff_borough\"]\n",
    "]\n",
    "\n",
    "# Step 2: Apply the indexers\n",
    "for indexer in indexers:\n",
    "    taxi_data_ml = indexer.fit(taxi_data_ml).transform(taxi_data_ml)\n",
    "\n",
    "# Step 3: One-hot encode the indexed columns\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"pickup_borough_index\", \"dropoff_borough_index\"],\n",
    "    outputCols=[\"pickup_borough_vec\", \"dropoff_borough_vec\"]\n",
    ")\n",
    "taxi_data_ml = encoder.fit(taxi_data_ml).transform(taxi_data_ml)\n",
    "\n",
    "# Show the encoded data\n",
    "taxi_data_ml.select(\n",
    "    \"pickup_borough\", \"pickup_borough_index\", \"pickup_borough_vec\",\n",
    "    \"dropoff_borough\", \"dropoff_borough_index\", \"dropoff_borough_vec\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bac0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|features_unscaled                                                                                     |features                                                                                                                                                                                                                                                                                                                                                                                                                         |high_tip|\n",
      "+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|(20,[0,1,2,3,4,5,6,7,8,14],[1.0,8.2,23.0,26.0,21.0,4.0,3.1707317073170733,1.1304347826086956,1.0,1.0])|[-0.49296136330894785,1.3902643874051224,0.08042453463909434,1.1788554188091773,1.1761911006811314,-0.05980669222865076,-0.09453704985608995,0.09851736042276807,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929]     |1       |\n",
      "|(20,[0,1,2,3,4,5,6,7,8,14],[1.0,2.0,20.0,13.5,11.0,7.0,6.75,0.675,1.0,1.0])                           |[-0.49296136330894785,-0.25198077643174127,0.03578985036367858,0.05283193759454141,-0.4559919230527556,1.4615278886829937,0.005864298442968333,-0.242885922590722,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929]    |1       |\n",
      "|(20,[0,1,2,3,4,5,6,7,8,14],[2.0,1.9,15.0,11.0,20.0,6.0,5.7894736842105265,0.7333333333333333,1.0,1.0])|[0.31448492731086003,-0.2784686016549165,-0.038601290095347685,-0.17237275864838578,1.0129727983077426,0.9544163617124456,-0.021079235624913078,-0.19915805579265372,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929] |1       |\n",
      "|(20,[0,1,2,3,4,5,6,7,8,14],[1.0,2.6,18.0,13.0,18.0,4.0,5.0,0.7222222222222222,1.0,1.0])               |[-0.49296136330894785,-0.0930538250926899,0.006033394180068072,0.007790998345955969,0.6865361935609653,-0.05980669222865076,-0.04322460609166494,-0.20748717327800006,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929]|0       |\n",
      "|(20,[0,1,2,3,4,5,6,7,8,14],[2.0,2.4,11.0,9.5,22.0,1.0,3.9583333333333335,0.8636363636363636,1.0,1.0]) |[0.31448492731086003,-0.1460294755390404,-0.0981142024625687,-0.3074955763941421,1.33940940305452,-1.5811412731402954,-0.07244419212418474,-0.101480223464501,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929]        |1       |\n",
      "+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Select feature columns\n",
    "feature_cols = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"trip_duration_mins\",\n",
    "    \"fare_amount\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_day_of_week\",\n",
    "    \"fare_per_mile\",\n",
    "    \"fare_per_minute\",\n",
    "    \"pickup_borough_vec\",\n",
    "    \"dropoff_borough_vec\"\n",
    "]\n",
    "\n",
    "# Assemble features into a single vector\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_unscaled\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "assembled_data = vector_assembler.transform(taxi_data_ml)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaled_data = scaler.fit(assembled_data).transform(assembled_data)\n",
    "\n",
    "# Show the assembled and scaled features\n",
    "scaled_data.select(\"features_unscaled\", \"features\", \"high_tip\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431328e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 20:56:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/05/23 20:56:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 721031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data count: 308375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[vendor_id: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: double, pickup_location_id: int, pickup_zone_name: string, pickup_borough: string, dropoff_location_id: int, dropoff_zone_name: string, dropoff_borough: string, is_inter_borough: boolean, trip_distance: double, trip_duration_mins: double, avg_speed_mph: double, fare_amount: double, tip_amount: double, tip_percentage: double, total_cost: double, cost_per_mile: double, pickup_hour: int, pickup_day: int, pickup_month: int, pickup_year: int, day_type: string, time_of_day: string, payment_type: int, rate_code: int, high_tip: int, pickup_day_of_week: int, fare_per_mile: double, fare_per_minute: double, pickup_borough_index: double, dropoff_borough_index: double, pickup_borough_vec: vector, dropoff_borough_vec: vector, features_unscaled: vector, features: vector]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and testing sets (70% training, 30% testing)\n",
    "train_data, test_data = scaled_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Training data count: {train_data.count()}\")\n",
    "print(f\"Testing data count: {test_data.count()}\")\n",
    "\n",
    "# Cache the datasets to improve performance\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db3fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|high_tip| count|\n",
      "+--------+------+\n",
      "|       0|439918|\n",
      "|       1|589488|\n",
      "+--------+------+\n",
      "\n",
      "+--------+------+-----------------+\n",
      "|high_tip| count|       proportion|\n",
      "+--------+------+-----------------+\n",
      "|       0|439918| 42.7351307453036|\n",
      "|       1|589488|57.26486925469639|\n",
      "+--------+------+-----------------+\n",
      "\n",
      "+--------+------+-----------------+\n",
      "|high_tip| count|       proportion|\n",
      "+--------+------+-----------------+\n",
      "|       0|439918| 42.7351307453036|\n",
      "|       1|589488|57.26486925469639|\n",
      "+--------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution of high_tip target\n",
    "class_counts = taxi_data_ml.groupBy(\"high_tip\").count().orderBy(\"high_tip\")\n",
    "class_counts.show()\n",
    "\n",
    "# Calculate class proportions\n",
    "total = taxi_data_ml.count()\n",
    "class_counts_with_proportions = class_counts.withColumn(\n",
    "    \"proportion\", \n",
    "    (col(\"count\") / total) * 100\n",
    ")\n",
    "class_counts_with_proportions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9dbea",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bead922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 20:56:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/05/23 20:56:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions from Logistic Regression:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|high_tip|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       1|       1.0|[0.42773472985211...|\n",
      "|       0|       1.0|[0.42773472985211...|\n",
      "|       1|       1.0|[0.42773472985211...|\n",
      "|       0|       1.0|[0.42773472985211...|\n",
      "|       0|       1.0|[0.42773472985211...|\n",
      "|       0|       1.0|[0.42773472985211...|\n",
      "|       1|       1.0|[0.42773472985211...|\n",
      "|       1|       1.0|[0.42773472985211...|\n",
      "|       0|       1.0|[0.42773472985211...|\n",
      "|       1|       1.0|[0.42773472985211...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Logistic Regression - Area under ROC: 0.5000\n",
      "Logistic Regression - Accuracy: 0.5735\n",
      "Logistic Regression - F1 Score: 0.4181\n",
      "Coefficients: (20,[],[])\n",
      "Intercept: 0.29109942950924583\n",
      "Logistic Regression - Area under ROC: 0.5000\n",
      "Logistic Regression - Accuracy: 0.5735\n",
      "Logistic Regression - F1 Score: 0.4181\n",
      "Coefficients: (20,[],[])\n",
      "Intercept: 0.29109942950924583\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Create and train the improved Logistic Regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_tip\",\n",
    "    maxIter=30,          # Increased from 10\n",
    "    regParam=0.1,        # Decreased from 0.3 for less regularization\n",
    "    elasticNetParam=0.5, # Changed from 0.8 for more L2 regularization\n",
    "    threshold=0.4,       # Adjusted for potential class imbalance\n",
    "    standardization=True # Ensure internal standardization\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample predictions from Logistic Regression:\")\n",
    "lr_predictions.select(\"high_tip\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Evaluate the model\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"high_tip\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "lr_auc = binary_evaluator.evaluate(lr_predictions)\n",
    "lr_accuracy = multiclass_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "# Also evaluate F1 score which is better for imbalanced datasets\n",
    "lr_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ").evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Logistic Regression - Area under ROC: {lr_auc:.4f}\")\n",
    "print(f\"Logistic Regression - Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression - F1 Score: {lr_f1:.4f}\")\n",
    "\n",
    "# Display model coefficients\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebd57a",
   "metadata": {},
   "source": [
    "## Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d771e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions from Decision Tree:\n",
      "+--------+----------+--------------------+\n",
      "|high_tip|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       1|       1.0|[0.44355979005189...|\n",
      "|       0|       1.0|[0.48743740474635...|\n",
      "|       1|       0.0|[0.58894127732533...|\n",
      "|       0|       1.0|[0.44355979005189...|\n",
      "|       0|       1.0|[0.44355979005189...|\n",
      "|       0|       1.0|[0.44355979005189...|\n",
      "|       1|       1.0|[0.48186169105916...|\n",
      "|       1|       1.0|[0.44355979005189...|\n",
      "|       0|       1.0|[0.44355979005189...|\n",
      "|       1|       1.0|[0.44355979005189...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Decision Tree - Area under ROC: 0.5201\n",
      "Decision Tree - Accuracy: 0.4749\n",
      "Decision Tree - F1 Score: 0.4749\n",
      "Feature Importances:\n",
      "passenger_count: 0.0010\n",
      "trip_distance: 0.1556\n",
      "trip_duration_mins: 0.0048\n",
      "fare_amount: 0.0684\n",
      "pickup_hour: 0.1334\n",
      "pickup_day_of_week: 0.0439\n",
      "fare_per_mile: 0.0922\n",
      "fare_per_minute: 0.0429\n",
      "pickup_borough_vec: 0.0225\n",
      "dropoff_borough_vec: 0.0673\n",
      "Decision Tree - Area under ROC: 0.5201\n",
      "Decision Tree - Accuracy: 0.4749\n",
      "Decision Tree - F1 Score: 0.4749\n",
      "Feature Importances:\n",
      "passenger_count: 0.0010\n",
      "trip_distance: 0.1556\n",
      "trip_duration_mins: 0.0048\n",
      "fare_amount: 0.0684\n",
      "pickup_hour: 0.1334\n",
      "pickup_day_of_week: 0.0439\n",
      "fare_per_mile: 0.0922\n",
      "fare_per_minute: 0.0429\n",
      "pickup_borough_vec: 0.0225\n",
      "dropoff_borough_vec: 0.0673\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Create and train the improved Decision Tree model\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_tip\",\n",
    "    maxDepth=8,         # Increased from 5\n",
    "    maxBins=64,         # Added parameter\n",
    "    minInstancesPerNode=10, # Added parameter\n",
    "    impurity=\"gini\",    # Specified impurity measure\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample predictions from Decision Tree:\")\n",
    "dt_predictions.select(\"high_tip\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Evaluate the model\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    rawPredictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "dt_accuracy = multiclass_evaluator.evaluate(dt_predictions)\n",
    "dt_auc = binary_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "dt_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ").evaluate(dt_predictions)\n",
    "\n",
    "print(f\"Decision Tree - Area under ROC: {dt_auc:.4f}\")\n",
    "print(f\"Decision Tree - Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Decision Tree - F1 Score: {dt_f1:.4f}\")\n",
    "\n",
    "# Display feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_cols, dt_model.featureImportances.toArray()):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43f499",
   "metadata": {},
   "source": [
    "## Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f261bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 20:56:57 WARN DAGScheduler: Broadcasting large task binary with size 1062.0 KiB\n",
      "25/05/23 20:56:57 WARN DAGScheduler: Broadcasting large task binary with size 1062.0 KiB\n",
      "25/05/23 20:57:11 WARN DAGScheduler: Broadcasting large task binary with size 1819.6 KiB\n",
      "25/05/23 20:57:11 WARN DAGScheduler: Broadcasting large task binary with size 1819.6 KiB\n",
      "25/05/23 20:57:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/23 20:57:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/23 20:57:43 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/05/23 20:57:43 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/05/23 20:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1230.1 KiB\n",
      "25/05/23 20:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1230.1 KiB\n",
      "25/05/23 20:58:04 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "25/05/23 20:58:04 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "25/05/23 20:58:24 WARN DAGScheduler: Broadcasting large task binary with size 1888.5 KiB\n",
      "25/05/23 20:58:24 WARN DAGScheduler: Broadcasting large task binary with size 1888.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions from Random Forest:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 20:58:27 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|high_tip|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       1|       1.0|[0.43958505217030...|\n",
      "|       0|       1.0|[0.47924848839585...|\n",
      "|       1|       0.0|[0.52984746281220...|\n",
      "|       0|       1.0|[0.42785397866077...|\n",
      "|       0|       1.0|[0.49014143122717...|\n",
      "|       0|       1.0|[0.43995671843351...|\n",
      "|       1|       1.0|[0.44708630199025...|\n",
      "|       1|       1.0|[0.44628107678114...|\n",
      "|       0|       1.0|[0.43948994194571...|\n",
      "|       1|       1.0|[0.42633943837007...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 20:58:27 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/23 20:58:31 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/23 20:58:31 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/23 20:58:34 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/23 20:58:34 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "[Stage 131:===================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Area under ROC: 0.5254\n",
      "Random Forest - Accuracy: 0.4844\n",
      "Random Forest - F1 Score: 0.4844\n",
      "Feature Importances:\n",
      "passenger_count: 0.0125\n",
      "trip_distance: 0.1206\n",
      "trip_duration_mins: 0.0537\n",
      "fare_amount: 0.0912\n",
      "pickup_hour: 0.1247\n",
      "pickup_day_of_week: 0.0462\n",
      "fare_per_mile: 0.0889\n",
      "fare_per_minute: 0.0793\n",
      "pickup_borough_vec: 0.0445\n",
      "dropoff_borough_vec: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Create and train the improved Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_tip\",\n",
    "    numTrees=100,      # Increased from 20\n",
    "    maxDepth=10,       # Increased from 5\n",
    "    minInstancesPerNode=5, # Added parameter\n",
    "    maxBins=128,       # Added parameter\n",
    "    bootstrap=True,    # Enable bootstrap sampling\n",
    "    impurity=\"gini\",   # Specified impurity measure\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample predictions from Random Forest:\")\n",
    "rf_predictions.select(\"high_tip\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Evaluate the model\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    rawPredictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rf_accuracy = multiclass_evaluator.evaluate(rf_predictions)\n",
    "rf_auc = binary_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "rf_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ").evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest - Area under ROC: {rf_auc:.4f}\")\n",
    "print(f\"Random Forest - Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest - F1 Score: {rf_f1:.4f}\")\n",
    "\n",
    "# Display feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_cols, rf_model.featureImportances.toArray()):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef631f",
   "metadata": {},
   "source": [
    "## Model Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fccf8",
   "metadata": {},
   "source": [
    "## Model 4: Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c11c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 20:59:34 WARN DAGScheduler: Broadcasting large task binary with size 1009.1 KiB\n",
      "25/05/23 20:59:34 WARN DAGScheduler: Broadcasting large task binary with size 1009.1 KiB\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_2, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_7, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_6, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_16, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_5, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_8, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_1, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_13, which does not exist\n",
      "25/05/23 20:59:34 WARN DAGScheduler: Broadcasting large task binary with size 1010.7 KiB\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_2, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_7, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_6, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_16, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_5, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_8, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_1, which does not exist\n",
      "25/05/23 20:59:34 WARN BlockManager: Asked to remove block rdd_1262_13, which does not exist\n",
      "25/05/23 20:59:34 WARN DAGScheduler: Broadcasting large task binary with size 1010.7 KiB\n",
      "25/05/23 20:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1011.2 KiB\n",
      "25/05/23 20:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1011.9 KiB\n",
      "25/05/23 20:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1011.2 KiB\n",
      "25/05/23 20:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1011.9 KiB\n",
      "25/05/23 20:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1012.9 KiB\n",
      "25/05/23 20:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1012.9 KiB\n",
      "25/05/23 20:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1014.9 KiB\n",
      "25/05/23 20:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1014.9 KiB\n",
      "25/05/23 20:59:36 WARN DAGScheduler: Broadcasting large task binary with size 1018.6 KiB\n",
      "25/05/23 20:59:36 WARN DAGScheduler: Broadcasting large task binary with size 1018.6 KiB\n",
      "25/05/23 20:59:36 WARN DAGScheduler: Broadcasting large task binary with size 1025.1 KiB\n",
      "25/05/23 20:59:36 WARN DAGScheduler: Broadcasting large task binary with size 1025.1 KiB\n",
      "25/05/23 20:59:36 WARN DAGScheduler: Broadcasting large task binary with size 1035.9 KiB\n",
      "25/05/23 20:59:36 WARN DAGScheduler: Broadcasting large task binary with size 1035.9 KiB\n",
      "25/05/23 20:59:37 WARN DAGScheduler: Broadcasting large task binary with size 1036.8 KiB\n",
      "25/05/23 20:59:37 WARN DAGScheduler: Broadcasting large task binary with size 1036.8 KiB\n",
      "25/05/23 20:59:37 WARN DAGScheduler: Broadcasting large task binary with size 1037.3 KiB\n",
      "25/05/23 20:59:37 WARN DAGScheduler: Broadcasting large task binary with size 1037.3 KiB\n",
      "25/05/23 20:59:37 WARN DAGScheduler: Broadcasting large task binary with size 1038.0 KiB\n",
      "25/05/23 20:59:37 WARN DAGScheduler: Broadcasting large task binary with size 1038.0 KiB\n",
      "25/05/23 20:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1039.0 KiB\n",
      "25/05/23 20:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1039.0 KiB\n",
      "25/05/23 20:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1041.2 KiB\n",
      "25/05/23 20:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1041.2 KiB\n",
      "25/05/23 20:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1045.8 KiB\n",
      "25/05/23 20:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1045.8 KiB\n",
      "25/05/23 20:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1053.5 KiB\n",
      "25/05/23 20:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1053.5 KiB\n",
      "25/05/23 20:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1068.5 KiB\n",
      "25/05/23 20:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1068.5 KiB\n",
      "25/05/23 20:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1069.3 KiB\n",
      "25/05/23 20:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1069.3 KiB\n",
      "25/05/23 20:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1069.8 KiB\n",
      "25/05/23 20:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1069.8 KiB\n",
      "25/05/23 20:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1070.5 KiB\n",
      "25/05/23 20:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1070.5 KiB\n",
      "25/05/23 20:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1071.5 KiB\n",
      "25/05/23 20:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1071.5 KiB\n",
      "25/05/23 20:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1073.8 KiB\n",
      "25/05/23 20:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1073.8 KiB\n",
      "25/05/23 20:59:41 WARN DAGScheduler: Broadcasting large task binary with size 1078.3 KiB\n",
      "25/05/23 20:59:41 WARN DAGScheduler: Broadcasting large task binary with size 1078.3 KiB\n",
      "25/05/23 20:59:41 WARN DAGScheduler: Broadcasting large task binary with size 1086.6 KiB\n",
      "25/05/23 20:59:41 WARN DAGScheduler: Broadcasting large task binary with size 1086.6 KiB\n",
      "25/05/23 20:59:41 WARN DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "25/05/23 20:59:41 WARN DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "25/05/23 20:59:42 WARN DAGScheduler: Broadcasting large task binary with size 1101.6 KiB\n",
      "25/05/23 20:59:42 WARN DAGScheduler: Broadcasting large task binary with size 1101.6 KiB\n",
      "25/05/23 20:59:42 WARN DAGScheduler: Broadcasting large task binary with size 1102.1 KiB\n",
      "25/05/23 20:59:42 WARN DAGScheduler: Broadcasting large task binary with size 1102.8 KiB\n",
      "25/05/23 20:59:42 WARN DAGScheduler: Broadcasting large task binary with size 1102.1 KiB\n",
      "25/05/23 20:59:42 WARN DAGScheduler: Broadcasting large task binary with size 1102.8 KiB\n",
      "25/05/23 20:59:42 WARN DAGScheduler: Broadcasting large task binary with size 1103.5 KiB\n",
      "25/05/23 20:59:42 WARN DAGScheduler: Broadcasting large task binary with size 1103.5 KiB\n",
      "25/05/23 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1105.2 KiB\n",
      "25/05/23 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1105.2 KiB\n",
      "25/05/23 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1108.3 KiB\n",
      "25/05/23 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1108.3 KiB\n",
      "25/05/23 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1114.3 KiB\n",
      "25/05/23 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1114.3 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1126.1 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1126.1 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1128.2 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1128.2 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1128.6 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1128.6 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1129.3 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1129.3 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1130.3 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1130.3 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1132.6 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1132.6 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1137.1 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1137.1 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1146.2 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1146.2 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1163.3 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1163.3 KiB\n",
      "25/05/23 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1167.0 KiB\n",
      "25/05/23 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1167.0 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1167.5 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1167.5 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1167.8 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1167.8 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1168.5 KiB\n",
      "25/05/23 20:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1168.5 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1169.5 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1169.5 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1171.5 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1171.5 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1175.2 KiB\n",
      "25/05/23 20:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1175.2 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1181.2 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1181.2 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1181.8 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1181.8 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1182.3 KiB\n",
      "25/05/23 20:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1182.3 KiB\n",
      "25/05/23 20:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1183.0 KiB\n",
      "25/05/23 20:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1183.0 KiB\n",
      "25/05/23 20:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1184.0 KiB\n",
      "25/05/23 20:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1184.0 KiB\n",
      "25/05/23 20:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1186.2 KiB\n",
      "25/05/23 20:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1186.2 KiB\n",
      "25/05/23 20:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1190.8 KiB\n",
      "25/05/23 20:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1190.8 KiB\n",
      "25/05/23 20:59:48 WARN DAGScheduler: Broadcasting large task binary with size 1199.9 KiB\n",
      "25/05/23 20:59:48 WARN DAGScheduler: Broadcasting large task binary with size 1199.9 KiB\n",
      "25/05/23 20:59:48 WARN DAGScheduler: Broadcasting large task binary with size 1217.0 KiB\n",
      "25/05/23 20:59:48 WARN DAGScheduler: Broadcasting large task binary with size 1217.0 KiB\n",
      "25/05/23 20:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1218.6 KiB\n",
      "25/05/23 20:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1218.6 KiB\n",
      "25/05/23 20:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1219.0 KiB\n",
      "25/05/23 20:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1219.0 KiB\n",
      "25/05/23 20:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1219.8 KiB\n",
      "25/05/23 20:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1219.8 KiB\n",
      "25/05/23 20:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1220.8 KiB\n",
      "25/05/23 20:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1220.8 KiB\n",
      "25/05/23 20:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1223.0 KiB\n",
      "25/05/23 20:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1223.0 KiB\n",
      "25/05/23 20:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1226.2 KiB\n",
      "25/05/23 20:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1226.2 KiB\n",
      "25/05/23 20:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1230.7 KiB\n",
      "25/05/23 20:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1230.7 KiB\n",
      "25/05/23 20:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1238.4 KiB\n",
      "25/05/23 20:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1238.4 KiB\n",
      "25/05/23 20:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1238.0 KiB\n",
      "25/05/23 20:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1238.0 KiB\n",
      "25/05/23 20:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1238.5 KiB\n",
      "25/05/23 20:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1238.5 KiB\n",
      "25/05/23 20:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1239.2 KiB\n",
      "25/05/23 20:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1239.2 KiB\n",
      "25/05/23 20:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1240.2 KiB\n",
      "25/05/23 20:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1240.2 KiB\n",
      "25/05/23 20:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1242.5 KiB\n",
      "25/05/23 20:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1242.5 KiB\n",
      "25/05/23 20:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1247.0 KiB\n",
      "25/05/23 20:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1247.0 KiB\n",
      "25/05/23 20:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1255.8 KiB\n",
      "25/05/23 20:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1255.8 KiB\n",
      "25/05/23 20:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1271.0 KiB\n",
      "25/05/23 20:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1271.0 KiB\n",
      "25/05/23 20:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1271.9 KiB\n",
      "25/05/23 20:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1271.9 KiB\n",
      "25/05/23 20:59:54 WARN DAGScheduler: Broadcasting large task binary with size 1272.4 KiB\n",
      "25/05/23 20:59:54 WARN DAGScheduler: Broadcasting large task binary with size 1272.4 KiB\n",
      "25/05/23 20:59:54 WARN DAGScheduler: Broadcasting large task binary with size 1273.1 KiB\n",
      "25/05/23 20:59:54 WARN DAGScheduler: Broadcasting large task binary with size 1273.1 KiB\n",
      "25/05/23 20:59:54 WARN DAGScheduler: Broadcasting large task binary with size 1273.8 KiB\n",
      "25/05/23 20:59:54 WARN DAGScheduler: Broadcasting large task binary with size 1273.8 KiB\n",
      "25/05/23 20:59:55 WARN DAGScheduler: Broadcasting large task binary with size 1275.5 KiB\n",
      "25/05/23 20:59:55 WARN DAGScheduler: Broadcasting large task binary with size 1275.5 KiB\n",
      "25/05/23 20:59:55 WARN DAGScheduler: Broadcasting large task binary with size 1277.9 KiB\n",
      "25/05/23 20:59:55 WARN DAGScheduler: Broadcasting large task binary with size 1277.9 KiB\n",
      "25/05/23 20:59:55 WARN DAGScheduler: Broadcasting large task binary with size 1280.8 KiB\n",
      "25/05/23 20:59:55 WARN DAGScheduler: Broadcasting large task binary with size 1280.8 KiB\n",
      "25/05/23 20:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1285.6 KiB\n",
      "25/05/23 20:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1285.6 KiB\n",
      "25/05/23 20:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1286.6 KiB\n",
      "25/05/23 20:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1286.6 KiB\n",
      "25/05/23 20:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1287.1 KiB\n",
      "25/05/23 20:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1287.1 KiB\n",
      "25/05/23 20:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1287.8 KiB\n",
      "25/05/23 20:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1287.8 KiB\n",
      "25/05/23 20:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1288.8 KiB\n",
      "25/05/23 20:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1288.8 KiB\n",
      "25/05/23 20:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1291.1 KiB\n",
      "25/05/23 20:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1291.1 KiB\n",
      "25/05/23 20:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1295.6 KiB\n",
      "25/05/23 20:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1295.6 KiB\n",
      "25/05/23 20:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1304.7 KiB\n",
      "25/05/23 20:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1304.7 KiB\n",
      "25/05/23 20:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1322.9 KiB\n",
      "25/05/23 20:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1322.9 KiB\n",
      "25/05/23 20:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1326.3 KiB\n",
      "25/05/23 20:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1326.3 KiB\n",
      "25/05/23 20:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1326.8 KiB\n",
      "25/05/23 20:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1326.8 KiB\n",
      "25/05/23 20:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1327.5 KiB\n",
      "25/05/23 20:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1327.5 KiB\n",
      "25/05/23 21:00:00 WARN DAGScheduler: Broadcasting large task binary with size 1328.5 KiB\n",
      "25/05/23 21:00:00 WARN DAGScheduler: Broadcasting large task binary with size 1328.5 KiB\n",
      "25/05/23 21:00:00 WARN DAGScheduler: Broadcasting large task binary with size 1330.8 KiB\n",
      "25/05/23 21:00:00 WARN DAGScheduler: Broadcasting large task binary with size 1330.8 KiB\n",
      "25/05/23 21:00:00 WARN DAGScheduler: Broadcasting large task binary with size 1334.0 KiB\n",
      "25/05/23 21:00:00 WARN DAGScheduler: Broadcasting large task binary with size 1334.0 KiB\n",
      "25/05/23 21:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1338.2 KiB\n",
      "25/05/23 21:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1338.2 KiB\n",
      "25/05/23 21:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1345.0 KiB\n",
      "25/05/23 21:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1345.0 KiB\n",
      "25/05/23 21:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1345.0 KiB\n",
      "25/05/23 21:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1345.0 KiB\n",
      "25/05/23 21:00:02 WARN DAGScheduler: Broadcasting large task binary with size 1345.4 KiB\n",
      "25/05/23 21:00:02 WARN DAGScheduler: Broadcasting large task binary with size 1345.4 KiB\n",
      "25/05/23 21:00:02 WARN DAGScheduler: Broadcasting large task binary with size 1346.1 KiB\n",
      "25/05/23 21:00:02 WARN DAGScheduler: Broadcasting large task binary with size 1346.1 KiB\n",
      "25/05/23 21:00:02 WARN DAGScheduler: Broadcasting large task binary with size 1347.1 KiB\n",
      "25/05/23 21:00:02 WARN DAGScheduler: Broadcasting large task binary with size 1347.1 KiB\n",
      "25/05/23 21:00:03 WARN DAGScheduler: Broadcasting large task binary with size 1349.4 KiB\n",
      "25/05/23 21:00:03 WARN DAGScheduler: Broadcasting large task binary with size 1349.4 KiB\n",
      "25/05/23 21:00:03 WARN DAGScheduler: Broadcasting large task binary with size 1353.4 KiB\n",
      "25/05/23 21:00:03 WARN DAGScheduler: Broadcasting large task binary with size 1353.4 KiB\n",
      "25/05/23 21:00:03 WARN DAGScheduler: Broadcasting large task binary with size 1360.8 KiB\n",
      "25/05/23 21:00:03 WARN DAGScheduler: Broadcasting large task binary with size 1360.8 KiB\n",
      "25/05/23 21:00:04 WARN DAGScheduler: Broadcasting large task binary with size 1373.9 KiB\n",
      "25/05/23 21:00:04 WARN DAGScheduler: Broadcasting large task binary with size 1373.9 KiB\n",
      "25/05/23 21:00:04 WARN DAGScheduler: Broadcasting large task binary with size 1374.9 KiB\n",
      "25/05/23 21:00:04 WARN DAGScheduler: Broadcasting large task binary with size 1374.9 KiB\n",
      "25/05/23 21:00:04 WARN DAGScheduler: Broadcasting large task binary with size 1375.4 KiB\n",
      "25/05/23 21:00:04 WARN DAGScheduler: Broadcasting large task binary with size 1375.4 KiB\n",
      "25/05/23 21:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1376.1 KiB\n",
      "25/05/23 21:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1376.1 KiB\n",
      "25/05/23 21:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1377.1 KiB\n",
      "25/05/23 21:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1377.1 KiB\n",
      "25/05/23 21:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1379.4 KiB\n",
      "25/05/23 21:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1379.4 KiB\n",
      "25/05/23 21:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1383.9 KiB\n",
      "25/05/23 21:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1383.9 KiB\n",
      "25/05/23 21:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1392.0 KiB\n",
      "25/05/23 21:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1392.0 KiB\n",
      "25/05/23 21:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1405.6 KiB\n",
      "25/05/23 21:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1405.6 KiB\n",
      "25/05/23 21:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1406.4 KiB\n",
      "25/05/23 21:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1406.4 KiB\n",
      "25/05/23 21:00:07 WARN DAGScheduler: Broadcasting large task binary with size 1406.9 KiB\n",
      "25/05/23 21:00:07 WARN DAGScheduler: Broadcasting large task binary with size 1406.9 KiB\n",
      "25/05/23 21:00:07 WARN DAGScheduler: Broadcasting large task binary with size 1407.6 KiB\n",
      "25/05/23 21:00:07 WARN DAGScheduler: Broadcasting large task binary with size 1407.6 KiB\n",
      "25/05/23 21:00:07 WARN DAGScheduler: Broadcasting large task binary with size 1408.6 KiB\n",
      "25/05/23 21:00:07 WARN DAGScheduler: Broadcasting large task binary with size 1408.6 KiB\n",
      "25/05/23 21:00:08 WARN DAGScheduler: Broadcasting large task binary with size 1410.6 KiB\n",
      "25/05/23 21:00:08 WARN DAGScheduler: Broadcasting large task binary with size 1410.6 KiB\n",
      "25/05/23 21:00:08 WARN DAGScheduler: Broadcasting large task binary with size 1414.5 KiB\n",
      "25/05/23 21:00:08 WARN DAGScheduler: Broadcasting large task binary with size 1414.5 KiB\n",
      "25/05/23 21:00:08 WARN DAGScheduler: Broadcasting large task binary with size 1422.2 KiB\n",
      "25/05/23 21:00:08 WARN DAGScheduler: Broadcasting large task binary with size 1422.2 KiB\n",
      "25/05/23 21:00:09 WARN DAGScheduler: Broadcasting large task binary with size 1436.4 KiB\n",
      "25/05/23 21:00:09 WARN DAGScheduler: Broadcasting large task binary with size 1436.4 KiB\n",
      "25/05/23 21:00:09 WARN DAGScheduler: Broadcasting large task binary with size 1436.8 KiB\n",
      "25/05/23 21:00:09 WARN DAGScheduler: Broadcasting large task binary with size 1436.8 KiB\n",
      "25/05/23 21:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1437.3 KiB\n",
      "25/05/23 21:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1437.3 KiB\n",
      "25/05/23 21:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1438.0 KiB\n",
      "25/05/23 21:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1438.0 KiB\n",
      "25/05/23 21:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1438.8 KiB\n",
      "25/05/23 21:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1438.8 KiB\n",
      "25/05/23 21:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1440.5 KiB\n",
      "25/05/23 21:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1440.5 KiB\n",
      "25/05/23 21:00:11 WARN DAGScheduler: Broadcasting large task binary with size 1443.6 KiB\n",
      "25/05/23 21:00:11 WARN DAGScheduler: Broadcasting large task binary with size 1443.6 KiB\n",
      "25/05/23 21:00:11 WARN DAGScheduler: Broadcasting large task binary with size 1449.3 KiB\n",
      "25/05/23 21:00:11 WARN DAGScheduler: Broadcasting large task binary with size 1449.3 KiB\n",
      "25/05/23 21:00:12 WARN DAGScheduler: Broadcasting large task binary with size 1457.8 KiB\n",
      "25/05/23 21:00:12 WARN DAGScheduler: Broadcasting large task binary with size 1457.8 KiB\n",
      "25/05/23 21:00:12 WARN DAGScheduler: Broadcasting large task binary with size 1457.1 KiB\n",
      "25/05/23 21:00:12 WARN DAGScheduler: Broadcasting large task binary with size 1457.1 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1457.6 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1457.6 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1458.3 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1458.3 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1459.3 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1459.3 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1461.6 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1461.6 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1465.8 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1465.8 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1474.1 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1474.1 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1488.9 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1488.9 KiB\n",
      "25/05/23 21:00:15 WARN DAGScheduler: Broadcasting large task binary with size 1489.0 KiB\n",
      "25/05/23 21:00:15 WARN DAGScheduler: Broadcasting large task binary with size 1489.0 KiB\n",
      "25/05/23 21:00:15 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n",
      "25/05/23 21:00:15 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1490.2 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1490.2 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1491.2 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1491.2 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1493.5 KiB\n",
      "25/05/23 21:00:13 WARN DAGScheduler: Broadcasting large task binary with size 1493.5 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1496.7 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1496.7 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1500.3 KiB\n",
      "25/05/23 21:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1500.3 KiB\n",
      "25/05/23 21:00:15 WARN DAGScheduler: Broadcasting large task binary with size 1505.7 KiB\n",
      "25/05/23 21:00:15 WARN DAGScheduler: Broadcasting large task binary with size 1505.7 KiB\n",
      "25/05/23 21:00:15 WARN DAGScheduler: Broadcasting large task binary with size 1505.7 KiB\n",
      "25/05/23 21:00:15 WARN DAGScheduler: Broadcasting large task binary with size 1505.7 KiB\n",
      "25/05/23 21:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1506.2 KiB\n",
      "25/05/23 21:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1506.2 KiB\n",
      "25/05/23 21:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1506.5 KiB\n",
      "25/05/23 21:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1506.5 KiB\n",
      "25/05/23 21:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1507.2 KiB\n",
      "25/05/23 21:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1507.2 KiB\n",
      "25/05/23 21:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1508.2 KiB\n",
      "25/05/23 21:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1508.2 KiB\n",
      "25/05/23 21:00:17 WARN DAGScheduler: Broadcasting large task binary with size 1510.5 KiB\n",
      "25/05/23 21:00:17 WARN DAGScheduler: Broadcasting large task binary with size 1510.5 KiB\n",
      "25/05/23 21:00:17 WARN DAGScheduler: Broadcasting large task binary with size 1514.5 KiB\n",
      "25/05/23 21:00:17 WARN DAGScheduler: Broadcasting large task binary with size 1514.5 KiB\n",
      "25/05/23 21:00:17 WARN DAGScheduler: Broadcasting large task binary with size 1520.3 KiB\n",
      "25/05/23 21:00:17 WARN DAGScheduler: Broadcasting large task binary with size 1520.3 KiB\n",
      "25/05/23 21:00:18 WARN DAGScheduler: Broadcasting large task binary with size 1520.5 KiB\n",
      "25/05/23 21:00:18 WARN DAGScheduler: Broadcasting large task binary with size 1520.5 KiB\n",
      "25/05/23 21:00:18 WARN DAGScheduler: Broadcasting large task binary with size 1520.9 KiB\n",
      "25/05/23 21:00:18 WARN DAGScheduler: Broadcasting large task binary with size 1520.9 KiB\n",
      "25/05/23 21:00:19 WARN DAGScheduler: Broadcasting large task binary with size 1521.7 KiB\n",
      "25/05/23 21:00:19 WARN DAGScheduler: Broadcasting large task binary with size 1521.7 KiB\n",
      "25/05/23 21:00:19 WARN DAGScheduler: Broadcasting large task binary with size 1522.7 KiB\n",
      "25/05/23 21:00:19 WARN DAGScheduler: Broadcasting large task binary with size 1522.7 KiB\n",
      "25/05/23 21:00:19 WARN DAGScheduler: Broadcasting large task binary with size 1524.1 KiB\n",
      "25/05/23 21:00:19 WARN DAGScheduler: Broadcasting large task binary with size 1524.1 KiB\n",
      "25/05/23 21:00:20 WARN DAGScheduler: Broadcasting large task binary with size 1527.0 KiB\n",
      "25/05/23 21:00:20 WARN DAGScheduler: Broadcasting large task binary with size 1527.0 KiB\n",
      "25/05/23 21:00:20 WARN DAGScheduler: Broadcasting large task binary with size 1531.3 KiB\n",
      "25/05/23 21:00:20 WARN DAGScheduler: Broadcasting large task binary with size 1531.3 KiB\n",
      "25/05/23 21:00:21 WARN DAGScheduler: Broadcasting large task binary with size 1538.2 KiB\n",
      "25/05/23 21:00:21 WARN DAGScheduler: Broadcasting large task binary with size 1538.2 KiB\n",
      "25/05/23 21:00:21 WARN DAGScheduler: Broadcasting large task binary with size 1538.5 KiB\n",
      "25/05/23 21:00:21 WARN DAGScheduler: Broadcasting large task binary with size 1538.5 KiB\n",
      "25/05/23 21:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1538.9 KiB\n",
      "25/05/23 21:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1538.9 KiB\n",
      "25/05/23 21:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1539.7 KiB\n",
      "25/05/23 21:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1539.7 KiB\n",
      "25/05/23 21:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1540.4 KiB\n",
      "25/05/23 21:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1540.4 KiB\n",
      "25/05/23 21:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1542.1 KiB\n",
      "25/05/23 21:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1542.1 KiB\n",
      "25/05/23 21:00:23 WARN DAGScheduler: Broadcasting large task binary with size 1544.4 KiB\n",
      "25/05/23 21:00:23 WARN DAGScheduler: Broadcasting large task binary with size 1544.4 KiB\n",
      "25/05/23 21:00:23 WARN DAGScheduler: Broadcasting large task binary with size 1547.4 KiB\n",
      "25/05/23 21:00:23 WARN DAGScheduler: Broadcasting large task binary with size 1547.4 KiB\n",
      "25/05/23 21:00:24 WARN DAGScheduler: Broadcasting large task binary with size 1552.5 KiB\n",
      "25/05/23 21:00:24 WARN DAGScheduler: Broadcasting large task binary with size 1552.5 KiB\n",
      "25/05/23 21:00:24 WARN DAGScheduler: Broadcasting large task binary with size 1552.7 KiB\n",
      "25/05/23 21:00:24 WARN DAGScheduler: Broadcasting large task binary with size 1552.7 KiB\n",
      "25/05/23 21:00:24 WARN DAGScheduler: Broadcasting large task binary with size 1553.2 KiB\n",
      "25/05/23 21:00:24 WARN DAGScheduler: Broadcasting large task binary with size 1553.2 KiB\n",
      "25/05/23 21:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1553.9 KiB\n",
      "25/05/23 21:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1553.9 KiB\n",
      "25/05/23 21:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1554.9 KiB\n",
      "25/05/23 21:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1554.9 KiB\n",
      "25/05/23 21:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1556.9 KiB\n",
      "25/05/23 21:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1556.9 KiB\n",
      "25/05/23 21:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1560.6 KiB\n",
      "25/05/23 21:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1560.6 KiB\n",
      "25/05/23 21:00:26 WARN DAGScheduler: Broadcasting large task binary with size 1566.4 KiB\n",
      "25/05/23 21:00:26 WARN DAGScheduler: Broadcasting large task binary with size 1566.4 KiB\n",
      "25/05/23 21:00:26 WARN DAGScheduler: Broadcasting large task binary with size 1577.6 KiB\n",
      "25/05/23 21:00:26 WARN DAGScheduler: Broadcasting large task binary with size 1577.6 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions from Gradient Boosted Trees:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 21:00:27 WARN DAGScheduler: Broadcasting large task binary with size 1545.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|high_tip|prediction|\n",
      "+--------+----------+\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 21:00:27 WARN DAGScheduler: Broadcasting large task binary with size 1564.7 KiB\n",
      "25/05/23 21:00:29 WARN DAGScheduler: Broadcasting large task binary with size 1553.5 KiB\n",
      "25/05/23 21:00:29 WARN DAGScheduler: Broadcasting large task binary with size 1553.5 KiB\n",
      "25/05/23 21:00:30 WARN DAGScheduler: Broadcasting large task binary with size 1564.7 KiB\n",
      "25/05/23 21:00:30 WARN DAGScheduler: Broadcasting large task binary with size 1564.7 KiB\n",
      "[Stage 951:=============>                                          (4 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees - Area under ROC: 0.5385\n",
      "Gradient Boosted Trees - Accuracy: 0.5253\n",
      "Gradient Boosted Trees - F1 Score: 0.5253\n",
      "Feature Importances:\n",
      "passenger_count: 0.0490\n",
      "trip_distance: 0.1047\n",
      "trip_duration_mins: 0.0778\n",
      "fare_amount: 0.1103\n",
      "pickup_hour: 0.1850\n",
      "pickup_day_of_week: 0.0955\n",
      "fare_per_mile: 0.1041\n",
      "fare_per_minute: 0.0952\n",
      "pickup_borough_vec: 0.0162\n",
      "dropoff_borough_vec: 0.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create and train the Gradient Boosted Trees model\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_tip\",\n",
    "    maxIter=50,        # Number of iterations\n",
    "    maxDepth=8,        # Tree depth\n",
    "    stepSize=0.1,      # Learning rate\n",
    "    minInstancesPerNode=10,\n",
    "    maxBins=64,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample predictions from Gradient Boosted Trees:\")\n",
    "gbt_predictions.select(\"high_tip\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluate the model\n",
    "gbt_accuracy = multiclass_evaluator.evaluate(gbt_predictions)\n",
    "gbt_auc = binary_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "gbt_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ").evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"Gradient Boosted Trees - Area under ROC: {gbt_auc:.4f}\")\n",
    "print(f\"Gradient Boosted Trees - Accuracy: {gbt_accuracy:.4f}\")\n",
    "print(f\"Gradient Boosted Trees - F1 Score: {gbt_f1:.4f}\")\n",
    "\n",
    "# Display feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_cols, gbt_model.featureImportances.toArray()):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b09386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "------------------------------------------------------------\n",
      "Model                     AUC        Accuracy   F1 Score  \n",
      "------------------------------------------------------------\n",
      "Logistic Regression       0.5000     0.5735     0.4181    \n",
      "Decision Tree             0.5201     0.4749     0.4749    \n",
      "Random Forest             0.5254     0.4844     0.4844    \n",
      "Gradient Boosted Trees    0.5385     0.5253     0.5253    \n",
      "\n",
      "Best model based on AUC: Gradient Boosted Trees with AUC = 0.5385\n",
      "Best model based on F1 Score: Gradient Boosted Trees with F1 = 0.5253\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "pickup_hour: 0.1850\n",
      "fare_amount: 0.1103\n",
      "trip_distance: 0.1047\n",
      "fare_per_mile: 0.1041\n",
      "pickup_day_of_week: 0.0955\n"
     ]
    }
   ],
   "source": [
    "# Compare model performances\n",
    "model_performance = {\n",
    "    \"Logistic Regression\": {\"AUC\": lr_auc, \"Accuracy\": lr_accuracy, \"F1\": lr_f1},\n",
    "    \"Decision Tree\": {\"AUC\": dt_auc, \"Accuracy\": dt_accuracy, \"F1\": dt_f1},\n",
    "    \"Random Forest\": {\"AUC\": rf_auc, \"Accuracy\": rf_accuracy, \"F1\": rf_f1},\n",
    "    \"Gradient Boosted Trees\": {\"AUC\": gbt_auc, \"Accuracy\": gbt_accuracy, \"F1\": gbt_f1}\n",
    "}\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Model':<25} {'AUC':<10} {'Accuracy':<10} {'F1 Score':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for model, metrics in model_performance.items():\n",
    "    print(f\"{model:<25} {metrics['AUC']:<10.4f} {metrics['Accuracy']:<10.4f} {metrics['F1']:<10.4f}\")\n",
    "\n",
    "# Find the best model based on AUC\n",
    "best_model_auc = max(model_performance.items(), key=lambda x: x[1]['AUC'])\n",
    "print(f\"\\nBest model based on AUC: {best_model_auc[0]} with AUC = {best_model_auc[1]['AUC']:.4f}\")\n",
    "\n",
    "# Find the best model based on F1 score (better for imbalanced datasets)\n",
    "best_model_f1 = max(model_performance.items(), key=lambda x: x[1]['F1'])\n",
    "print(f\"Best model based on F1 Score: {best_model_f1[0]} with F1 = {best_model_f1[1]['F1']:.4f}\")\n",
    "\n",
    "# Analyze important features from the best model\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "if best_model_auc[0] == \"Random Forest\" or best_model_f1[0] == \"Random Forest\":\n",
    "    rf_importances = sorted(list(zip(feature_cols, rf_model.featureImportances.toArray())), key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in rf_importances[:5]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "elif best_model_auc[0] == \"Gradient Boosted Trees\" or best_model_f1[0] == \"Gradient Boosted Trees\":\n",
    "    gbt_importances = sorted(list(zip(feature_cols, gbt_model.featureImportances.toArray())), key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in gbt_importances[:5]:\n",
    "        print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841041ee",
   "metadata": {},
   "source": [
    "## Optimizing Prediction Threshold\n",
    "\n",
    "Prompt used: \"Review the notebook and propose areas for improvement. Optimize one of the 3 models implemented to increase its accuracy even further\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d7b369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Gradient Boosted Trees doesn't support probability predictions.\n",
      "Using raw prediction scores for threshold optimization instead.\n",
      "\n",
      "Threshold optimization not applicable for Gradient Boosted Trees\n",
      "GBT uses internal threshold optimization during training.\n",
      "Current metrics for Gradient Boosted Trees:\n",
      "Accuracy: 0.5253\n",
      "F1 Score: 0.5253\n"
     ]
    }
   ],
   "source": [
    "# Let's optimize the threshold for models that support probability predictions\n",
    "# GBT doesn't support probability predictions, so we'll use a different approach\n",
    "best_model_name = best_model_auc[0]\n",
    "best_predictions = None\n",
    "\n",
    "if best_model_name == \"Logistic Regression\":\n",
    "    best_predictions = lr_predictions\n",
    "elif best_model_name == \"Random Forest\":\n",
    "    best_predictions = rf_predictions\n",
    "elif best_model_name == \"Decision Tree\":\n",
    "    best_predictions = dt_predictions\n",
    "elif best_model_name == \"Gradient Boosted Trees\":\n",
    "    print(f\"Note: {best_model_name} doesn't support probability predictions.\")\n",
    "    print(\"Using raw prediction scores for threshold optimization instead.\")\n",
    "    best_predictions = gbt_predictions\n",
    "\n",
    "if best_predictions is not None:\n",
    "    # Check if the model supports probability predictions\n",
    "    has_probability = \"probability\" in best_predictions.columns\n",
    "    \n",
    "    if has_probability and best_model_name != \"Gradient Boosted Trees\":\n",
    "        # For models with probability predictions (LR, RF, DT)\n",
    "        def evaluate_threshold(threshold):\n",
    "            from pyspark.sql.functions import when, col\n",
    "            from pyspark.ml.linalg import VectorUDT\n",
    "            from pyspark.sql.functions import udf\n",
    "            from pyspark.sql.types import DoubleType\n",
    "            \n",
    "            # Extract probability of positive class using UDF\n",
    "            def extract_prob(probability_vector):\n",
    "                if probability_vector is not None:\n",
    "                    return float(probability_vector[1])  # probability of class 1\n",
    "                return 0.0\n",
    "            \n",
    "            extract_prob_udf = udf(extract_prob, DoubleType())\n",
    "            \n",
    "            # Create custom prediction using threshold\n",
    "            threshold_predictions = best_predictions.withColumn(\n",
    "                \"prob_positive\",\n",
    "                extract_prob_udf(col(\"probability\"))\n",
    "            ).withColumn(\n",
    "                \"custom_prediction\",\n",
    "                when(col(\"prob_positive\") > threshold, 1.0).otherwise(0.0)\n",
    "            )\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"high_tip\",\n",
    "                predictionCol=\"custom_prediction\",\n",
    "                metricName=\"accuracy\"\n",
    "            ).evaluate(threshold_predictions)\n",
    "            \n",
    "            f1 = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"high_tip\",\n",
    "                predictionCol=\"custom_prediction\",\n",
    "                metricName=\"f1\"\n",
    "            ).evaluate(threshold_predictions)\n",
    "            \n",
    "            return threshold, accuracy, f1\n",
    "            \n",
    "    else:\n",
    "        # For GBT - use raw prediction scores\n",
    "        def evaluate_threshold(threshold):\n",
    "            from pyspark.sql.functions import when, col\n",
    "            \n",
    "            # For GBT, we can't optimize threshold in the same way\n",
    "            # since it doesn't provide probability scores\n",
    "            # We'll return the original metrics\n",
    "            accuracy = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"high_tip\",\n",
    "                predictionCol=\"prediction\",\n",
    "                metricName=\"accuracy\"\n",
    "            ).evaluate(best_predictions)\n",
    "            \n",
    "            f1 = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"high_tip\",\n",
    "                predictionCol=\"prediction\",\n",
    "                metricName=\"f1\"\n",
    "            ).evaluate(best_predictions)\n",
    "            \n",
    "            return threshold, accuracy, f1\n",
    "    \n",
    "    # Test different thresholds\n",
    "    if best_model_name != \"Gradient Boosted Trees\":\n",
    "        thresholds = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "        print(f\"\\nOptimizing threshold for {best_model_name}...\")\n",
    "        threshold_metrics = [evaluate_threshold(t) for t in thresholds]\n",
    "        \n",
    "        # Print threshold evaluation results\n",
    "        print(\"\\nThreshold Optimization Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Threshold':<10} {'Accuracy':<10} {'F1 Score':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        for threshold, accuracy, f1 in threshold_metrics:\n",
    "            print(f\"{threshold:<10.2f} {accuracy:<10.4f} {f1:<10.4f}\")\n",
    "        \n",
    "        # Find best threshold\n",
    "        best_threshold = max(threshold_metrics, key=lambda x: x[2])[0]\n",
    "        print(f\"\\nBest threshold: {best_threshold} (optimized for F1 score)\")\n",
    "        \n",
    "        # Apply the best threshold for final evaluation\n",
    "        from pyspark.sql.functions import udf\n",
    "        from pyspark.sql.types import DoubleType\n",
    "        \n",
    "        def extract_prob(probability_vector):\n",
    "            if probability_vector is not None:\n",
    "                return float(probability_vector[1])\n",
    "            return 0.0\n",
    "        \n",
    "        extract_prob_udf = udf(extract_prob, DoubleType())\n",
    "        \n",
    "        optimized_predictions = best_predictions.withColumn(\n",
    "            \"prob_positive\",\n",
    "            extract_prob_udf(col(\"probability\"))\n",
    "        ).withColumn(\n",
    "            \"optimized_prediction\",\n",
    "            when(col(\"prob_positive\") > best_threshold, 1.0).otherwise(0.0)\n",
    "        )\n",
    "        \n",
    "        # Calculate final metrics with optimized threshold\n",
    "        opt_accuracy = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"high_tip\",\n",
    "            predictionCol=\"optimized_prediction\",\n",
    "            metricName=\"accuracy\"\n",
    "        ).evaluate(optimized_predictions)\n",
    "        \n",
    "        opt_f1 = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"high_tip\",\n",
    "            predictionCol=\"optimized_prediction\",\n",
    "            metricName=\"f1\"\n",
    "        ).evaluate(optimized_predictions)\n",
    "        \n",
    "        print(f\"\\nFinal optimized metrics for {best_model_name}:\")\n",
    "        print(f\"Accuracy: {opt_accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {opt_f1:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # For GBT, show that threshold optimization isn't applicable\n",
    "        print(f\"\\nThreshold optimization not applicable for {best_model_name}\")\n",
    "        print(\"GBT uses internal threshold optimization during training.\")\n",
    "        print(f\"Current metrics for {best_model_name}:\")\n",
    "        print(f\"Accuracy: {gbt_accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {gbt_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
