{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6e6ec5",
   "metadata": {},
   "source": [
    "#### SparkML Task: Trip Profiling: Predict Likelihood of High Tipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cb1266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configure Spark environment\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'\n",
    "\n",
    "# Create a Spark session with improved configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"NYC Taxi Model\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"20\") \\\n",
    "    .config(\"spark.default.parallelism\", \"20\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark session\n",
    "print(\"Spark session created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cbe6053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+------------------+--------------------+--------------+-------------------+--------------------+---------------+----------------+-------------+------------------+------------------+-----------+----------+------------------+------------------+------------------+-----------+----------+------------+-----------+--------+------------+------------+---------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|pickup_location_id|    pickup_zone_name|pickup_borough|dropoff_location_id|   dropoff_zone_name|dropoff_borough|is_inter_borough|trip_distance|trip_duration_mins|     avg_speed_mph|fare_amount|tip_amount|    tip_percentage|        total_cost|     cost_per_mile|pickup_hour|pickup_day|pickup_month|pickup_year|day_type| time_of_day|payment_type|rate_code|\n",
      "+---------+-------------------+-------------------+---------------+------------------+--------------------+--------------+-------------------+--------------------+---------------+----------------+-------------+------------------+------------------+-----------+----------+------------------+------------------+------------------+-----------+----------+------------+-----------+--------+------------+------------+---------+\n",
      "|        1|2018-06-13 21:38:00|2018-06-13 22:01:00|            1.0|               229|Sutton Place/Turt...|     Manhattan|                244|Washington Height...|      Manhattan|           false|          8.2|              23.0|21.391304347826082|       26.0|      5.45|20.961538461538463|             32.75|3.1707317073170733|         21|         4|           6|       2018| weekday|       night|           1|        1|\n",
      "|        1|2018-12-01 11:40:00|2018-12-01 12:00:00|            1.0|                79|        East Village|     Manhattan|                231|TriBeCa/Civic Center|      Manhattan|           false|          2.0|              20.0|               6.0|       13.5|      2.15|15.925925925925924|             16.45|              6.75|         11|         7|          12|       2018| weekend|      midday|           1|        1|\n",
      "|        1|2018-03-09 20:51:00|2018-03-09 21:06:00|            2.0|               161|      Midtown Center|     Manhattan|                249|        West Village|      Manhattan|           false|          1.9|              15.0|               7.6|       11.0|      2.45|22.272727272727273|             14.75|5.7894736842105265|         20|         6|           3|       2018| weekday|       night|           1|        1|\n",
      "|        1|2018-02-21 18:26:00|2018-02-21 18:44:00|            1.0|               233| UN/Turtle Bay South|     Manhattan|                262|      Yorkville East|      Manhattan|           false|          2.6|              18.0| 8.666666666666668|       13.0|       0.0|               0.0|              14.8|               5.0|         18|         4|           2|       2018| weekday|evening_rush|           2|        1|\n",
      "|        1|2018-08-12 22:02:00|2018-08-12 22:13:00|            2.0|               162|        Midtown East|     Manhattan|                140|     Lenox Hill East|      Manhattan|           false|          2.4|              11.0|13.090909090909092|        9.5|       2.7|28.421052631578945|              13.5|3.9583333333333335|         22|         1|           8|       2018| weekend|       night|           1|        1|\n",
      "|        1|2018-07-25 09:23:00|2018-07-25 09:27:00|            1.0|               125|           Hudson Sq|     Manhattan|                144| Little Italy/NoLiTa|      Manhattan|           false|          0.5|               4.0|               7.5|        4.5|      1.05|23.333333333333332|              6.35|               9.0|          9|         4|           7|       2018| weekday|morning_rush|           1|        1|\n",
      "|        1|2018-03-08 08:52:00|2018-03-08 09:11:00|            1.0|               236|Upper East Side N...|     Manhattan|                 88|Financial Distric...|      Manhattan|           false|          7.4|              19.0|23.368421052631582|       22.5|      4.65|20.666666666666668|             27.95|3.0405405405405403|          8|         5|           3|       2018| weekday|morning_rush|           1|        1|\n",
      "|        1|2018-12-18 22:10:00|2018-12-18 22:25:00|            1.0|                13|   Battery Park City|     Manhattan|                233| UN/Turtle Bay South|      Manhattan|           false|          6.1|              15.0|              24.4|       19.5|       1.0| 5.128205128205128|              21.8|  3.19672131147541|         22|         3|          12|       2018| weekday|       night|           1|        1|\n",
      "|        1|2018-03-09 22:39:00|2018-03-09 23:03:00|            1.0|                79|        East Village|     Manhattan|                141|     Lenox Hill West|      Manhattan|           false|          3.6|              24.0|               9.0|       17.5|       1.0| 5.714285714285714|              19.8| 4.861111111111111|         22|         6|           3|       2018| weekday|       night|           1|        1|\n",
      "|        2|2018-03-08 09:15:00|2018-03-08 09:25:00|            1.0|               186|Penn Station/Madi...|     Manhattan|                230|Times Sq/Theatre ...|      Manhattan|           false|         0.85|              10.0|5.1000000000000005|        7.5|      1.24| 16.53333333333333| 9.540000000000001| 8.823529411764707|          9|         5|           3|       2018| weekday|morning_rush|           1|        1|\n",
      "|        1|2018-01-17 08:45:00|2018-01-17 08:55:00|            1.0|                75|   East Harlem South|     Manhattan|                142| Lincoln Square East|      Manhattan|           false|          2.3|              10.0|13.799999999999999|        9.5|       1.0|10.526315789473683|              11.3| 4.130434782608696|          8|         4|           1|       2018| weekday|morning_rush|           1|        1|\n",
      "|        2|2018-08-12 15:10:00|2018-08-12 15:18:00|            1.0|                24|        Bloomingdale|     Manhattan|                239|Upper West Side S...|      Manhattan|           false|         1.65|               8.0|            12.375|        8.0|       0.0|               0.0|               8.8| 4.848484848484849|         15|         1|           8|       2018| weekend|      midday|           2|        1|\n",
      "|        1|2018-02-03 16:31:00|2018-02-03 16:38:00|            1.0|               142| Lincoln Square East|     Manhattan|                230|Times Sq/Theatre ...|      Manhattan|           false|          0.7|               7.0| 5.999999999999999|        6.5|       0.0|               0.0|               7.3| 9.285714285714286|         16|         7|           2|       2018| weekend|evening_rush|           2|        1|\n",
      "|        1|2018-01-03 21:40:00|2018-01-03 21:54:00|            1.0|               114|Greenwich Village...|     Manhattan|                 66|  DUMBO/Vinegar Hill|       Brooklyn|            true|          2.8|              14.0|11.999999999999998|       12.5|      2.75|              22.0|             16.55| 4.464285714285714|         21|         4|           1|       2018| weekday|       night|           1|        1|\n",
      "|        1|2018-08-21 08:35:00|2018-08-21 08:40:00|            1.0|               162|        Midtown East|     Manhattan|                163|       Midtown North|      Manhattan|           false|          0.5|               5.0|               6.0|        5.0|      0.75|              15.0|              6.55|              10.0|          8|         3|           8|       2018| weekday|morning_rush|           1|        1|\n",
      "|        2|2018-06-06 22:07:00|2018-06-06 22:39:00|            1.0|               162|        Midtown East|     Manhattan|                 49|        Clinton Hill|       Brooklyn|            true|         8.91|              32.0|          16.70625|       30.0|       4.0|13.333333333333334|              35.3|3.3670033670033668|         22|         4|           6|       2018| weekday|       night|           1|        1|\n",
      "|        2|2018-11-16 01:19:00|2018-11-16 01:55:00|            2.0|               132|         JFK Airport|        Queens|                237|Upper East Side S...|      Manhattan|            true|        20.66|              36.0| 34.43333333333334|       52.0|       0.0|               0.0|58.559999999999995|2.5169409486931267|          1|         6|          11|       2018| weekday|       night|           2|        2|\n",
      "|        1|2018-01-09 16:12:00|2018-01-09 16:19:00|            1.0|               141|     Lenox Hill West|     Manhattan|                237|Upper East Side S...|      Manhattan|           false|          0.5|               7.0| 4.285714285714286|        5.5|       0.0|               0.0|               7.3|              11.0|         16|         3|           1|       2018| weekday|evening_rush|           2|        1|\n",
      "|        1|2018-02-06 20:08:00|2018-02-06 20:15:00|            1.0|               161|      Midtown Center|     Manhattan|                164|       Midtown South|      Manhattan|           false|          1.1|               7.0| 9.428571428571429|        6.5|      1.95|              30.0|              9.75| 5.909090909090908|         20|         3|           2|       2018| weekday|       night|           1|        1|\n",
      "|        2|2018-01-27 21:40:00|2018-01-27 21:56:00|            3.0|               237|Upper East Side S...|     Manhattan|                113|Greenwich Village...|      Manhattan|           false|         2.64|              16.0|               9.9|       12.0|       0.0|               0.0|              13.3| 4.545454545454545|         21|         7|           1|       2018| weekend|       night|           2|        1|\n",
      "+---------+-------------------+-------------------+---------------+------------------+--------------------+--------------+-------------------+--------------------+---------------+----------------+-------------+------------------+------------------+-----------+----------+------------------+------------------+------------------+-----------+----------+------------+-----------+--------+------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_data = spark.read.parquet(\"/root/DevDataOps/nyc-taxi-analysis/processed-data/nyc_taxi_processed.parquet\")\n",
    "taxi_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b9879",
   "metadata": {},
   "source": [
    "## Setting Up Models For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fd0cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "root\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- pickup_zone_name: string (nullable = true)\n",
      " |-- pickup_borough: string (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- dropoff_zone_name: string (nullable = true)\n",
      " |-- dropoff_borough: string (nullable = true)\n",
      " |-- is_inter_borough: boolean (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- trip_duration_mins: double (nullable = true)\n",
      " |-- avg_speed_mph: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tip_percentage: double (nullable = true)\n",
      " |-- total_cost: double (nullable = true)\n",
      " |-- cost_per_mile: double (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_day: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- day_type: string (nullable = true)\n",
      " |-- time_of_day: string (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Copy Parquet File to not tamper with the original one\n",
    "taxi_data_copy = taxi_data\n",
    "\n",
    "# taxi_data_copy.show()\n",
    "num_columns = len(taxi_data.columns)\n",
    "print(num_columns)\n",
    "\n",
    "\n",
    "taxi_data_copy.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f79367d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+---------------+-------------+------------------+-----------+------------------+------------------+--------------+---------------+----------+--------+\n",
      "|pickup_hour|pickup_day_of_week|passenger_count|trip_distance|trip_duration_mins|fare_amount|     fare_per_mile|   fare_per_minute|pickup_borough|dropoff_borough|tip_amount|high_tip|\n",
      "+-----------+------------------+---------------+-------------+------------------+-----------+------------------+------------------+--------------+---------------+----------+--------+\n",
      "|         21|                 4|            1.0|          8.2|              23.0|       26.0|3.1707317073170733|1.1304347826086956|     Manhattan|      Manhattan|      5.45|       1|\n",
      "|         11|                 7|            1.0|          2.0|              20.0|       13.5|              6.75|             0.675|     Manhattan|      Manhattan|      2.15|       1|\n",
      "|         20|                 6|            2.0|          1.9|              15.0|       11.0|5.7894736842105265|0.7333333333333333|     Manhattan|      Manhattan|      2.45|       1|\n",
      "|         18|                 4|            1.0|          2.6|              18.0|       13.0|               5.0|0.7222222222222222|     Manhattan|      Manhattan|       0.0|       0|\n",
      "|         22|                 1|            2.0|          2.4|              11.0|        9.5|3.9583333333333335|0.8636363636363636|     Manhattan|      Manhattan|       2.7|       1|\n",
      "+-----------+------------------+---------------+-------------+------------------+-----------+------------------+------------------+--------------+---------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, hour, dayofweek, expr\n",
    "\n",
    "# Create binary target column for high_tip (1 if tip_amount > 0.15 * fare_amount, 0 otherwise)\n",
    "taxi_data_ml = taxi_data_copy.withColumn(\n",
    "    \"high_tip\", \n",
    "    when(col(\"tip_amount\") > (0.15 * col(\"fare_amount\")), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Feature engineering\n",
    "# 1. Extract time features\n",
    "taxi_data_ml = taxi_data_ml.withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\")))\n",
    "taxi_data_ml = taxi_data_ml.withColumn(\"pickup_day_of_week\", dayofweek(col(\"pickup_datetime\")))\n",
    "\n",
    "# 2. Create fare_per_mile feature\n",
    "taxi_data_ml = taxi_data_ml.withColumn(\n",
    "    \"fare_per_mile\",\n",
    "    when(col(\"trip_distance\") > 0, col(\"fare_amount\") / col(\"trip_distance\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# 3. Calculate fare_per_minute feature\n",
    "taxi_data_ml = taxi_data_ml.withColumn(\n",
    "    \"fare_per_minute\",\n",
    "    when(col(\"trip_duration_mins\") > 0, col(\"fare_amount\") / col(\"trip_duration_mins\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# Display the first few rows with the new features\n",
    "taxi_data_ml.select(\n",
    "    \"pickup_hour\", \"pickup_day_of_week\", \"passenger_count\", \"trip_distance\", \n",
    "    \"trip_duration_mins\", \"fare_amount\", \"fare_per_mile\", \"fare_per_minute\",\n",
    "    \"pickup_borough\", \"dropoff_borough\", \"tip_amount\", \"high_tip\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2664cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------------------+---------------+---------------------+-------------------+\n",
      "|pickup_borough|pickup_borough_index|pickup_borough_vec|dropoff_borough|dropoff_borough_index|dropoff_borough_vec|\n",
      "+--------------+--------------------+------------------+---------------+---------------------+-------------------+\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "|     Manhattan|                 0.0|     (6,[0],[1.0])|      Manhattan|                  0.0|      (6,[0],[1.0])|\n",
      "+--------------+--------------------+------------------+---------------+---------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Handle categorical variables: pickup_borough and dropoff_borough\n",
    "# Step 1: Convert string columns to indices\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
    "    for col_name in [\"pickup_borough\", \"dropoff_borough\"]\n",
    "]\n",
    "\n",
    "# Step 2: Apply the indexers\n",
    "for indexer in indexers:\n",
    "    taxi_data_ml = indexer.fit(taxi_data_ml).transform(taxi_data_ml)\n",
    "\n",
    "# Step 3: One-hot encode the indexed columns\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"pickup_borough_index\", \"dropoff_borough_index\"],\n",
    "    outputCols=[\"pickup_borough_vec\", \"dropoff_borough_vec\"]\n",
    ")\n",
    "taxi_data_ml = encoder.fit(taxi_data_ml).transform(taxi_data_ml)\n",
    "\n",
    "# Show the encoded data\n",
    "taxi_data_ml.select(\n",
    "    \"pickup_borough\", \"pickup_borough_index\", \"pickup_borough_vec\",\n",
    "    \"dropoff_borough\", \"dropoff_borough_index\", \"dropoff_borough_vec\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|features_unscaled                                                                                            |features                                                                                                                                                                                                                                                                                                                                                                                                                                            |high_tip|\n",
      "+-------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|(21,[0,1,2,3,4,5,6,7,8,14,20],[1.0,8.2,23.0,26.0,21.0,4.0,3.1707317073170733,1.1304347826086956,1.0,1.0,1.0])|[-0.49296136330894785,1.3902643874051224,0.08042453463909434,1.1788554188091773,1.1761911006811314,-0.05980669222865076,-0.09453704985608995,0.09851736042276807,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929,-0.6485127021147479]    |1       |\n",
      "|(21,[0,1,2,3,4,5,6,7,8,14,20],[1.0,2.0,20.0,13.5,11.0,7.0,6.75,0.675,1.0,1.0,1.0])                           |[-0.49296136330894785,-0.25198077643174127,0.03578985036367858,0.05283193759454141,-0.4559919230527556,1.4615278886829937,0.005864298442968333,-0.242885922590722,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929,-0.6485127021147479]   |1       |\n",
      "|(21,[0,1,2,3,4,5,6,7,8,14,20],[2.0,1.9,15.0,11.0,20.0,6.0,5.7894736842105265,0.7333333333333333,1.0,1.0,1.0])|[0.31448492731086003,-0.2784686016549165,-0.038601290095347685,-0.17237275864838578,1.0129727983077426,0.9544163617124456,-0.021079235624913078,-0.19915805579265372,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929,-0.6485127021147479]|1       |\n",
      "|(21,[0,1,2,3,4,5,6,7,8,14,20],[1.0,2.6,18.0,13.0,18.0,4.0,5.0,0.7222222222222222,1.0,1.0,2.0])               |[-0.49296136330894785,-0.0930538250926899,0.006033394180068072,0.007790998345955969,0.6865361935609653,-0.05980669222865076,-0.04322460609166494,-0.20748717327800006,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929,1.4468970106947783]|0       |\n",
      "|(21,[0,1,2,3,4,5,6,7,8,14,20],[2.0,2.4,11.0,9.5,22.0,1.0,3.9583333333333335,0.8636363636363636,1.0,1.0,1.0]) |[0.31448492731086003,-0.1460294755390404,-0.0981142024625687,-0.3074955763941421,1.33940940305452,-1.5811412731402954,-0.07244419212418474,-0.101480223464501,0.31589414863002035,-0.25811126613137914,-0.11229349190743218,-0.033179403583591824,-0.00406382023238001,-0.00406382023238001,0.3569893961722316,-0.22194605595005365,-0.21105577792004027,-0.07988117862617265,-0.04355358222435769,-0.015429120508601929,-0.6485127021147479]       |1       |\n",
      "+-------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Select feature columns\n",
    "feature_cols = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"trip_duration_mins\",\n",
    "    \"fare_amount\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_day_of_week\",\n",
    "    \"fare_per_mile\",\n",
    "    \"fare_per_minute\",\n",
    "    \"pickup_borough_vec\",\n",
    "    \"dropoff_borough_vec\",\n",
    "    \"payment_type\"  # Added Payment Type\n",
    "]\n",
    "\n",
    "# Assemble features into a single vector\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_unscaled\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "assembled_data = vector_assembler.transform(taxi_data_ml)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaled_data = scaler.fit(assembled_data).transform(assembled_data)\n",
    "\n",
    "# Show the assembled and scaled features\n",
    "scaled_data.select(\"features_unscaled\", \"features\", \"high_tip\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "431328e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 721031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 970:================>                                       (5 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data count: 308375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[vendor_id: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: double, pickup_location_id: int, pickup_zone_name: string, pickup_borough: string, dropoff_location_id: int, dropoff_zone_name: string, dropoff_borough: string, is_inter_borough: boolean, trip_distance: double, trip_duration_mins: double, avg_speed_mph: double, fare_amount: double, tip_amount: double, tip_percentage: double, total_cost: double, cost_per_mile: double, pickup_hour: int, pickup_day: int, pickup_month: int, pickup_year: int, day_type: string, time_of_day: string, payment_type: int, rate_code: int, high_tip: int, pickup_day_of_week: int, fare_per_mile: double, fare_per_minute: double, pickup_borough_index: double, dropoff_borough_index: double, pickup_borough_vec: vector, dropoff_borough_vec: vector, features_unscaled: vector, features: vector]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and testing sets (70% training, 30% testing)\n",
    "train_data, test_data = scaled_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Training data count: {train_data.count()}\")\n",
    "print(f\"Testing data count: {test_data.count()}\")\n",
    "\n",
    "# Cache the datasets to improve performance\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1db3fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|high_tip| count|\n",
      "+--------+------+\n",
      "|       0|439918|\n",
      "|       1|589488|\n",
      "+--------+------+\n",
      "\n",
      "+--------+------+-----------------+\n",
      "|high_tip| count|       proportion|\n",
      "+--------+------+-----------------+\n",
      "|       0|439918| 42.7351307453036|\n",
      "|       1|589488|57.26486925469639|\n",
      "+--------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution of high_tip target\n",
    "class_counts = taxi_data_ml.groupBy(\"high_tip\").count().orderBy(\"high_tip\")\n",
    "class_counts.show()\n",
    "\n",
    "# Calculate class proportions\n",
    "total = taxi_data_ml.count()\n",
    "class_counts_with_proportions = class_counts.withColumn(\n",
    "    \"proportion\", \n",
    "    (col(\"count\") / total) * 100\n",
    ")\n",
    "class_counts_with_proportions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9dbea",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bead922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions from Logistic Regression:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|high_tip|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       1|       1.0|[0.25455326922552...|\n",
      "|       0|       1.0|[0.25455326922552...|\n",
      "|       1|       1.0|[0.25455326922552...|\n",
      "|       0|       1.0|[0.25455326922552...|\n",
      "|       0|       1.0|[0.25455326922552...|\n",
      "|       0|       0.0|[0.82168725221710...|\n",
      "|       1|       1.0|[0.25455326922552...|\n",
      "|       1|       1.0|[0.25455326922552...|\n",
      "|       0|       0.0|[0.82168725221710...|\n",
      "|       1|       1.0|[0.25455326922552...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Area under ROC: 0.8547\n",
      "Logistic Regression - Accuracy: 0.8761\n",
      "Logistic Regression - F1 Score: 0.8716\n",
      "Coefficients: (21,[20],[-1.2419024166453427])\n",
      "Intercept: 0.26908406188758877\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Create and train the improved Logistic Regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_tip\",\n",
    "    maxIter=30,          # Increased from 10\n",
    "    regParam=0.1,        # Decreased from 0.3 for less regularization\n",
    "    elasticNetParam=0.5, # Changed from 0.8 for more L2 regularization\n",
    "    threshold=0.4,       # Adjusted for potential class imbalance\n",
    "    standardization=True # Ensure internal standardization\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample predictions from Logistic Regression:\")\n",
    "lr_predictions.select(\"high_tip\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Evaluate the model\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"high_tip\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "lr_auc = binary_evaluator.evaluate(lr_predictions)\n",
    "lr_accuracy = multiclass_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "# Also evaluate F1 score which is better for imbalanced datasets\n",
    "lr_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ").evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Logistic Regression - Area under ROC: {lr_auc:.4f}\")\n",
    "print(f\"Logistic Regression - Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression - F1 Score: {lr_f1:.4f}\")\n",
    "\n",
    "# Display model coefficients\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebd57a",
   "metadata": {},
   "source": [
    "## Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d771e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions from Decision Tree:\n",
      "+--------+----------+--------------------+\n",
      "|high_tip|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       1|       1.0|[0.24040914256850...|\n",
      "|       0|       1.0|[0.24040914256850...|\n",
      "|       1|       1.0|[0.24040914256850...|\n",
      "|       0|       1.0|[0.21839183985949...|\n",
      "|       0|       1.0|[0.24040914256850...|\n",
      "|       0|       0.0|[0.99994068937199...|\n",
      "|       1|       1.0|[0.11249784816663...|\n",
      "|       1|       1.0|[0.24040914256850...|\n",
      "|       0|       0.0|[0.99994068937199...|\n",
      "|       1|       1.0|[0.21839183985949...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Decision Tree - Area under ROC: 0.8556\n",
      "Decision Tree - Accuracy: 0.8722\n",
      "Decision Tree - F1 Score: 0.8722\n",
      "Feature Importances:\n",
      "passenger_count: 0.0000\n",
      "trip_distance: 0.0000\n",
      "trip_duration_mins: 0.0006\n",
      "fare_amount: 0.0154\n",
      "pickup_hour: 0.0006\n",
      "pickup_day_of_week: 0.0000\n",
      "fare_per_mile: 0.0002\n",
      "fare_per_minute: 0.0001\n",
      "pickup_borough_vec: 0.0000\n",
      "dropoff_borough_vec: 0.0007\n",
      "payment_type: 0.0005\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Create and train the improved Decision Tree model\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_tip\",\n",
    "    maxDepth=8,         # Increased from 5\n",
    "    maxBins=64,         # Added parameter\n",
    "    minInstancesPerNode=10, # Added parameter\n",
    "    impurity=\"gini\",    # Specified impurity measure\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample predictions from Decision Tree:\")\n",
    "dt_predictions.select(\"high_tip\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Evaluate the model\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    rawPredictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "dt_accuracy = multiclass_evaluator.evaluate(dt_predictions)\n",
    "dt_auc = binary_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "dt_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ").evaluate(dt_predictions)\n",
    "\n",
    "print(f\"Decision Tree - Area under ROC: {dt_auc:.4f}\")\n",
    "print(f\"Decision Tree - Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Decision Tree - F1 Score: {dt_f1:.4f}\")\n",
    "\n",
    "# Display feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_cols, dt_model.featureImportances.toArray()):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43f499",
   "metadata": {},
   "source": [
    "## Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f261bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 23:00:54 WARN DAGScheduler: Broadcasting large task binary with size 1525.1 KiB\n",
      "25/05/23 23:01:15 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/23 23:01:44 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/05/23 23:02:18 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/05/23 23:02:53 WARN DAGScheduler: Broadcasting large task binary with size 1182.7 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions from Random Forest:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 23:02:52 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|high_tip|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       1|       1.0|[0.24921694381596...|\n",
      "|       0|       1.0|[0.26819814216803...|\n",
      "|       1|       1.0|[0.23831019510976...|\n",
      "|       0|       1.0|[0.21672361186197...|\n",
      "|       0|       1.0|[0.29064978043296...|\n",
      "|       0|       0.0|[0.96699949866913...|\n",
      "|       1|       1.0|[0.12211167092472...|\n",
      "|       1|       1.0|[0.25181373912848...|\n",
      "|       0|       0.0|[0.96699624419378...|\n",
      "|       1|       1.0|[0.21604001703933...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 23:02:53 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/05/23 23:02:56 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/05/23 23:02:59 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "[Stage 1102:=============================>                         (9 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Area under ROC: 0.8556\n",
      "Random Forest - Accuracy: 0.8723\n",
      "Random Forest - F1 Score: 0.8723\n",
      "Feature Importances:\n",
      "passenger_count: 0.0002\n",
      "trip_distance: 0.0038\n",
      "trip_duration_mins: 0.0035\n",
      "fare_amount: 0.0066\n",
      "pickup_hour: 0.0021\n",
      "pickup_day_of_week: 0.0005\n",
      "fare_per_mile: 0.0021\n",
      "fare_per_minute: 0.0018\n",
      "pickup_borough_vec: 0.0007\n",
      "dropoff_borough_vec: 0.0011\n",
      "payment_type: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Create and train the improved Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_tip\",\n",
    "    numTrees=100,      # Increased from 20\n",
    "    maxDepth=10,       # Increased from 5\n",
    "    minInstancesPerNode=5, # Added parameter\n",
    "    maxBins=128,       # Added parameter\n",
    "    bootstrap=True,    # Enable bootstrap sampling\n",
    "    impurity=\"gini\",   # Specified impurity measure\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample predictions from Random Forest:\")\n",
    "rf_predictions.select(\"high_tip\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Evaluate the model\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    rawPredictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rf_accuracy = multiclass_evaluator.evaluate(rf_predictions)\n",
    "rf_auc = binary_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "rf_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ").evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest - Area under ROC: {rf_auc:.4f}\")\n",
    "print(f\"Random Forest - Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest - F1 Score: {rf_f1:.4f}\")\n",
    "\n",
    "# Display feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_cols, rf_model.featureImportances.toArray()):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef631f",
   "metadata": {},
   "source": [
    "## Model Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fccf8",
   "metadata": {},
   "source": [
    "## Model 4: Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9c11c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 23:03:52 WARN DAGScheduler: Broadcasting large task binary with size 1006.0 KiB\n",
      "25/05/23 23:03:53 WARN DAGScheduler: Broadcasting large task binary with size 1008.1 KiB\n",
      "25/05/23 23:03:54 WARN DAGScheduler: Broadcasting large task binary with size 1008.5 KiB\n",
      "25/05/23 23:03:54 WARN DAGScheduler: Broadcasting large task binary with size 1009.2 KiB\n",
      "25/05/23 23:03:51 WARN DAGScheduler: Broadcasting large task binary with size 1010.2 KiB\n",
      "25/05/23 23:03:51 WARN DAGScheduler: Broadcasting large task binary with size 1012.5 KiB\n",
      "25/05/23 23:03:51 WARN DAGScheduler: Broadcasting large task binary with size 1016.8 KiB\n",
      "25/05/23 23:03:52 WARN DAGScheduler: Broadcasting large task binary with size 1024.5 KiB\n",
      "25/05/23 23:03:52 WARN DAGScheduler: Broadcasting large task binary with size 1038.4 KiB\n",
      "25/05/23 23:03:52 WARN DAGScheduler: Broadcasting large task binary with size 1040.5 KiB\n",
      "25/05/23 23:03:53 WARN DAGScheduler: Broadcasting large task binary with size 1041.0 KiB\n",
      "25/05/23 23:03:53 WARN DAGScheduler: Broadcasting large task binary with size 1041.7 KiB\n",
      "25/05/23 23:03:53 WARN DAGScheduler: Broadcasting large task binary with size 1042.7 KiB\n",
      "25/05/23 23:03:53 WARN DAGScheduler: Broadcasting large task binary with size 1044.9 KiB\n",
      "25/05/23 23:03:54 WARN DAGScheduler: Broadcasting large task binary with size 1049.2 KiB\n",
      "25/05/23 23:03:54 WARN DAGScheduler: Broadcasting large task binary with size 1056.6 KiB\n",
      "25/05/23 23:03:54 WARN DAGScheduler: Broadcasting large task binary with size 1070.8 KiB\n",
      "25/05/23 23:03:54 WARN DAGScheduler: Broadcasting large task binary with size 1074.0 KiB\n",
      "25/05/23 23:03:55 WARN DAGScheduler: Broadcasting large task binary with size 1074.5 KiB\n",
      "25/05/23 23:03:55 WARN DAGScheduler: Broadcasting large task binary with size 1075.2 KiB\n",
      "25/05/23 23:03:55 WARN DAGScheduler: Broadcasting large task binary with size 1076.2 KiB\n",
      "25/05/23 23:03:56 WARN DAGScheduler: Broadcasting large task binary with size 1078.5 KiB\n",
      "25/05/23 23:03:56 WARN DAGScheduler: Broadcasting large task binary with size 1082.7 KiB\n",
      "25/05/23 23:03:56 WARN DAGScheduler: Broadcasting large task binary with size 1090.7 KiB\n",
      "25/05/23 23:03:56 WARN DAGScheduler: Broadcasting large task binary with size 1105.0 KiB\n",
      "25/05/23 23:03:57 WARN DAGScheduler: Broadcasting large task binary with size 1106.4 KiB\n",
      "25/05/23 23:03:57 WARN DAGScheduler: Broadcasting large task binary with size 1106.9 KiB\n",
      "25/05/23 23:03:58 WARN DAGScheduler: Broadcasting large task binary with size 1107.6 KiB\n",
      "25/05/23 23:03:58 WARN DAGScheduler: Broadcasting large task binary with size 1108.6 KiB\n",
      "25/05/23 23:03:58 WARN DAGScheduler: Broadcasting large task binary with size 1110.9 KiB\n",
      "25/05/23 23:03:58 WARN DAGScheduler: Broadcasting large task binary with size 1115.2 KiB\n",
      "25/05/23 23:03:59 WARN DAGScheduler: Broadcasting large task binary with size 1122.6 KiB\n",
      "25/05/23 23:03:59 WARN DAGScheduler: Broadcasting large task binary with size 1137.0 KiB\n",
      "25/05/23 23:03:59 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "25/05/23 23:04:00 WARN DAGScheduler: Broadcasting large task binary with size 1140.1 KiB\n",
      "25/05/23 23:04:00 WARN DAGScheduler: Broadcasting large task binary with size 1140.8 KiB\n",
      "25/05/23 23:04:00 WARN DAGScheduler: Broadcasting large task binary with size 1141.8 KiB\n",
      "25/05/23 23:04:01 WARN DAGScheduler: Broadcasting large task binary with size 1144.1 KiB\n",
      "25/05/23 23:04:01 WARN DAGScheduler: Broadcasting large task binary with size 1148.4 KiB\n",
      "25/05/23 23:04:01 WARN DAGScheduler: Broadcasting large task binary with size 1156.3 KiB\n",
      "25/05/23 23:04:01 WARN DAGScheduler: Broadcasting large task binary with size 1169.5 KiB\n",
      "25/05/23 23:04:02 WARN DAGScheduler: Broadcasting large task binary with size 1171.1 KiB\n",
      "25/05/23 23:04:02 WARN DAGScheduler: Broadcasting large task binary with size 1171.6 KiB\n",
      "25/05/23 23:04:02 WARN DAGScheduler: Broadcasting large task binary with size 1172.3 KiB\n",
      "25/05/23 23:04:03 WARN DAGScheduler: Broadcasting large task binary with size 1173.3 KiB\n",
      "25/05/23 23:04:03 WARN DAGScheduler: Broadcasting large task binary with size 1175.5 KiB\n",
      "25/05/23 23:04:03 WARN DAGScheduler: Broadcasting large task binary with size 1179.6 KiB\n",
      "25/05/23 23:04:03 WARN DAGScheduler: Broadcasting large task binary with size 1186.7 KiB\n",
      "25/05/23 23:04:04 WARN DAGScheduler: Broadcasting large task binary with size 1200.0 KiB\n",
      "25/05/23 23:04:04 WARN DAGScheduler: Broadcasting large task binary with size 1201.9 KiB\n",
      "25/05/23 23:04:05 WARN DAGScheduler: Broadcasting large task binary with size 1202.4 KiB\n",
      "25/05/23 23:04:05 WARN DAGScheduler: Broadcasting large task binary with size 1203.1 KiB\n",
      "25/05/23 23:04:05 WARN DAGScheduler: Broadcasting large task binary with size 1204.1 KiB\n",
      "25/05/23 23:04:05 WARN DAGScheduler: Broadcasting large task binary with size 1206.3 KiB\n",
      "25/05/23 23:04:06 WARN DAGScheduler: Broadcasting large task binary with size 1210.3 KiB\n",
      "25/05/23 23:04:06 WARN DAGScheduler: Broadcasting large task binary with size 1217.8 KiB\n",
      "25/05/23 23:04:06 WARN DAGScheduler: Broadcasting large task binary with size 1231.4 KiB\n",
      "25/05/23 23:04:07 WARN DAGScheduler: Broadcasting large task binary with size 1232.8 KiB\n",
      "25/05/23 23:04:07 WARN DAGScheduler: Broadcasting large task binary with size 1233.3 KiB\n",
      "25/05/23 23:04:07 WARN DAGScheduler: Broadcasting large task binary with size 1234.0 KiB\n",
      "25/05/23 23:04:08 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n",
      "25/05/23 23:04:08 WARN DAGScheduler: Broadcasting large task binary with size 1237.3 KiB\n",
      "25/05/23 23:04:08 WARN DAGScheduler: Broadcasting large task binary with size 1241.6 KiB\n",
      "25/05/23 23:04:09 WARN DAGScheduler: Broadcasting large task binary with size 1249.5 KiB\n",
      "25/05/23 23:04:09 WARN DAGScheduler: Broadcasting large task binary with size 1263.2 KiB\n",
      "25/05/23 23:04:09 WARN DAGScheduler: Broadcasting large task binary with size 1264.9 KiB\n",
      "25/05/23 23:04:10 WARN DAGScheduler: Broadcasting large task binary with size 1265.4 KiB\n",
      "25/05/23 23:04:10 WARN DAGScheduler: Broadcasting large task binary with size 1266.1 KiB\n",
      "25/05/23 23:04:10 WARN DAGScheduler: Broadcasting large task binary with size 1267.1 KiB\n",
      "25/05/23 23:04:10 WARN DAGScheduler: Broadcasting large task binary with size 1269.4 KiB\n",
      "25/05/23 23:04:11 WARN DAGScheduler: Broadcasting large task binary with size 1273.4 KiB\n",
      "25/05/23 23:04:11 WARN DAGScheduler: Broadcasting large task binary with size 1280.0 KiB\n",
      "25/05/23 23:04:11 WARN DAGScheduler: Broadcasting large task binary with size 1292.4 KiB\n",
      "25/05/23 23:04:11 WARN DAGScheduler: Broadcasting large task binary with size 1293.9 KiB\n",
      "25/05/23 23:04:12 WARN DAGScheduler: Broadcasting large task binary with size 1294.4 KiB\n",
      "25/05/23 23:04:12 WARN DAGScheduler: Broadcasting large task binary with size 1295.1 KiB\n",
      "25/05/23 23:04:12 WARN DAGScheduler: Broadcasting large task binary with size 1296.1 KiB\n",
      "25/05/23 23:04:13 WARN DAGScheduler: Broadcasting large task binary with size 1298.4 KiB\n",
      "25/05/23 23:04:13 WARN DAGScheduler: Broadcasting large task binary with size 1302.4 KiB\n",
      "25/05/23 23:04:13 WARN DAGScheduler: Broadcasting large task binary with size 1310.1 KiB\n",
      "25/05/23 23:04:13 WARN DAGScheduler: Broadcasting large task binary with size 1322.9 KiB\n",
      "25/05/23 23:04:14 WARN DAGScheduler: Broadcasting large task binary with size 1323.9 KiB\n",
      "25/05/23 23:04:14 WARN DAGScheduler: Broadcasting large task binary with size 1324.4 KiB\n",
      "25/05/23 23:04:14 WARN DAGScheduler: Broadcasting large task binary with size 1325.1 KiB\n",
      "25/05/23 23:04:15 WARN DAGScheduler: Broadcasting large task binary with size 1326.1 KiB\n",
      "25/05/23 23:04:15 WARN DAGScheduler: Broadcasting large task binary with size 1328.4 KiB\n",
      "25/05/23 23:04:15 WARN DAGScheduler: Broadcasting large task binary with size 1332.6 KiB\n",
      "25/05/23 23:04:15 WARN DAGScheduler: Broadcasting large task binary with size 1340.1 KiB\n",
      "25/05/23 23:04:16 WARN DAGScheduler: Broadcasting large task binary with size 1354.0 KiB\n",
      "25/05/23 23:04:16 WARN DAGScheduler: Broadcasting large task binary with size 1355.5 KiB\n",
      "25/05/23 23:04:17 WARN DAGScheduler: Broadcasting large task binary with size 1356.0 KiB\n",
      "25/05/23 23:04:17 WARN DAGScheduler: Broadcasting large task binary with size 1356.7 KiB\n",
      "25/05/23 23:04:17 WARN DAGScheduler: Broadcasting large task binary with size 1357.7 KiB\n",
      "25/05/23 23:04:18 WARN DAGScheduler: Broadcasting large task binary with size 1360.0 KiB\n",
      "25/05/23 23:04:18 WARN DAGScheduler: Broadcasting large task binary with size 1364.2 KiB\n",
      "25/05/23 23:04:18 WARN DAGScheduler: Broadcasting large task binary with size 1372.2 KiB\n",
      "25/05/23 23:04:18 WARN DAGScheduler: Broadcasting large task binary with size 1386.2 KiB\n",
      "25/05/23 23:04:19 WARN DAGScheduler: Broadcasting large task binary with size 1388.6 KiB\n",
      "25/05/23 23:04:19 WARN DAGScheduler: Broadcasting large task binary with size 1389.1 KiB\n",
      "25/05/23 23:04:20 WARN DAGScheduler: Broadcasting large task binary with size 1389.8 KiB\n",
      "25/05/23 23:04:20 WARN DAGScheduler: Broadcasting large task binary with size 1390.8 KiB\n",
      "25/05/23 23:04:20 WARN DAGScheduler: Broadcasting large task binary with size 1393.0 KiB\n",
      "25/05/23 23:04:21 WARN DAGScheduler: Broadcasting large task binary with size 1397.3 KiB\n",
      "25/05/23 23:04:21 WARN DAGScheduler: Broadcasting large task binary with size 1405.0 KiB\n",
      "25/05/23 23:04:21 WARN DAGScheduler: Broadcasting large task binary with size 1420.3 KiB\n",
      "25/05/23 23:04:21 WARN DAGScheduler: Broadcasting large task binary with size 1422.8 KiB\n",
      "25/05/23 23:04:22 WARN DAGScheduler: Broadcasting large task binary with size 1423.3 KiB\n",
      "25/05/23 23:04:22 WARN DAGScheduler: Broadcasting large task binary with size 1424.0 KiB\n",
      "25/05/23 23:04:22 WARN DAGScheduler: Broadcasting large task binary with size 1425.0 KiB\n",
      "25/05/23 23:04:23 WARN DAGScheduler: Broadcasting large task binary with size 1427.2 KiB\n",
      "25/05/23 23:04:23 WARN DAGScheduler: Broadcasting large task binary with size 1431.0 KiB\n",
      "25/05/23 23:04:20 WARN DAGScheduler: Broadcasting large task binary with size 1438.1 KiB\n",
      "25/05/23 23:04:21 WARN DAGScheduler: Broadcasting large task binary with size 1450.0 KiB\n",
      "25/05/23 23:04:21 WARN DAGScheduler: Broadcasting large task binary with size 1450.0 KiB\n",
      "25/05/23 23:04:22 WARN DAGScheduler: Broadcasting large task binary with size 1450.5 KiB\n",
      "25/05/23 23:04:22 WARN DAGScheduler: Broadcasting large task binary with size 1451.2 KiB\n",
      "25/05/23 23:04:22 WARN DAGScheduler: Broadcasting large task binary with size 1452.2 KiB\n",
      "25/05/23 23:04:23 WARN DAGScheduler: Broadcasting large task binary with size 1454.5 KiB\n",
      "25/05/23 23:04:23 WARN DAGScheduler: Broadcasting large task binary with size 1458.7 KiB\n",
      "25/05/23 23:04:23 WARN DAGScheduler: Broadcasting large task binary with size 1466.2 KiB\n",
      "25/05/23 23:04:23 WARN DAGScheduler: Broadcasting large task binary with size 1479.5 KiB\n",
      "25/05/23 23:04:24 WARN DAGScheduler: Broadcasting large task binary with size 1481.1 KiB\n",
      "25/05/23 23:04:24 WARN DAGScheduler: Broadcasting large task binary with size 1481.6 KiB\n",
      "25/05/23 23:04:24 WARN DAGScheduler: Broadcasting large task binary with size 1482.3 KiB\n",
      "25/05/23 23:04:25 WARN DAGScheduler: Broadcasting large task binary with size 1483.3 KiB\n",
      "25/05/23 23:04:25 WARN DAGScheduler: Broadcasting large task binary with size 1485.6 KiB\n",
      "25/05/23 23:04:25 WARN DAGScheduler: Broadcasting large task binary with size 1489.6 KiB\n",
      "25/05/23 23:04:26 WARN DAGScheduler: Broadcasting large task binary with size 1496.7 KiB\n",
      "25/05/23 23:04:26 WARN DAGScheduler: Broadcasting large task binary with size 1510.3 KiB\n",
      "25/05/23 23:04:26 WARN DAGScheduler: Broadcasting large task binary with size 1511.9 KiB\n",
      "25/05/23 23:04:27 WARN DAGScheduler: Broadcasting large task binary with size 1512.4 KiB\n",
      "25/05/23 23:04:27 WARN DAGScheduler: Broadcasting large task binary with size 1513.1 KiB\n",
      "25/05/23 23:04:27 WARN DAGScheduler: Broadcasting large task binary with size 1514.1 KiB\n",
      "25/05/23 23:04:27 WARN DAGScheduler: Broadcasting large task binary with size 1516.4 KiB\n",
      "25/05/23 23:04:28 WARN DAGScheduler: Broadcasting large task binary with size 1520.6 KiB\n",
      "25/05/23 23:04:28 WARN DAGScheduler: Broadcasting large task binary with size 1528.9 KiB\n",
      "25/05/23 23:04:28 WARN DAGScheduler: Broadcasting large task binary with size 1542.9 KiB\n",
      "25/05/23 23:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1543.2 KiB\n",
      "25/05/23 23:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1543.7 KiB\n",
      "25/05/23 23:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1544.4 KiB\n",
      "25/05/23 23:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1545.4 KiB\n",
      "25/05/23 23:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1547.6 KiB\n",
      "25/05/23 23:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1551.9 KiB\n",
      "25/05/23 23:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1559.6 KiB\n",
      "25/05/23 23:04:31 WARN DAGScheduler: Broadcasting large task binary with size 1574.4 KiB\n",
      "25/05/23 23:04:31 WARN DAGScheduler: Broadcasting large task binary with size 1576.9 KiB\n",
      "25/05/23 23:04:32 WARN DAGScheduler: Broadcasting large task binary with size 1577.4 KiB\n",
      "25/05/23 23:04:32 WARN DAGScheduler: Broadcasting large task binary with size 1578.1 KiB\n",
      "25/05/23 23:04:32 WARN DAGScheduler: Broadcasting large task binary with size 1579.1 KiB\n",
      "25/05/23 23:04:32 WARN DAGScheduler: Broadcasting large task binary with size 1581.4 KiB\n",
      "25/05/23 23:04:33 WARN DAGScheduler: Broadcasting large task binary with size 1585.1 KiB\n",
      "25/05/23 23:04:33 WARN DAGScheduler: Broadcasting large task binary with size 1592.2 KiB\n",
      "25/05/23 23:04:33 WARN DAGScheduler: Broadcasting large task binary with size 1604.2 KiB\n",
      "25/05/23 23:04:34 WARN DAGScheduler: Broadcasting large task binary with size 1605.9 KiB\n",
      "25/05/23 23:04:34 WARN DAGScheduler: Broadcasting large task binary with size 1606.4 KiB\n",
      "25/05/23 23:04:34 WARN DAGScheduler: Broadcasting large task binary with size 1607.1 KiB\n",
      "25/05/23 23:04:35 WARN DAGScheduler: Broadcasting large task binary with size 1608.1 KiB\n",
      "25/05/23 23:04:35 WARN DAGScheduler: Broadcasting large task binary with size 1610.4 KiB\n",
      "25/05/23 23:04:35 WARN DAGScheduler: Broadcasting large task binary with size 1614.6 KiB\n",
      "25/05/23 23:04:35 WARN DAGScheduler: Broadcasting large task binary with size 1622.6 KiB\n",
      "25/05/23 23:04:36 WARN DAGScheduler: Broadcasting large task binary with size 1637.1 KiB\n",
      "25/05/23 23:04:36 WARN DAGScheduler: Broadcasting large task binary with size 1638.3 KiB\n",
      "25/05/23 23:04:37 WARN DAGScheduler: Broadcasting large task binary with size 1638.8 KiB\n",
      "25/05/23 23:04:37 WARN DAGScheduler: Broadcasting large task binary with size 1639.5 KiB\n",
      "25/05/23 23:04:37 WARN DAGScheduler: Broadcasting large task binary with size 1640.5 KiB\n",
      "25/05/23 23:04:38 WARN DAGScheduler: Broadcasting large task binary with size 1642.8 KiB\n",
      "25/05/23 23:04:38 WARN DAGScheduler: Broadcasting large task binary with size 1646.8 KiB\n",
      "25/05/23 23:04:38 WARN DAGScheduler: Broadcasting large task binary with size 1654.2 KiB\n",
      "25/05/23 23:04:38 WARN DAGScheduler: Broadcasting large task binary with size 1667.0 KiB\n",
      "25/05/23 23:04:39 WARN DAGScheduler: Broadcasting large task binary with size 1668.5 KiB\n",
      "25/05/23 23:04:39 WARN DAGScheduler: Broadcasting large task binary with size 1668.9 KiB\n",
      "25/05/23 23:04:39 WARN DAGScheduler: Broadcasting large task binary with size 1669.7 KiB\n",
      "25/05/23 23:04:40 WARN DAGScheduler: Broadcasting large task binary with size 1670.7 KiB\n",
      "25/05/23 23:04:40 WARN DAGScheduler: Broadcasting large task binary with size 1672.9 KiB\n",
      "25/05/23 23:04:40 WARN DAGScheduler: Broadcasting large task binary with size 1677.2 KiB\n",
      "25/05/23 23:04:41 WARN DAGScheduler: Broadcasting large task binary with size 1684.9 KiB\n",
      "25/05/23 23:04:41 WARN DAGScheduler: Broadcasting large task binary with size 1698.0 KiB\n",
      "25/05/23 23:04:41 WARN DAGScheduler: Broadcasting large task binary with size 1698.1 KiB\n",
      "25/05/23 23:04:42 WARN DAGScheduler: Broadcasting large task binary with size 1698.6 KiB\n",
      "25/05/23 23:04:42 WARN DAGScheduler: Broadcasting large task binary with size 1699.3 KiB\n",
      "25/05/23 23:04:42 WARN DAGScheduler: Broadcasting large task binary with size 1700.3 KiB\n",
      "25/05/23 23:04:43 WARN DAGScheduler: Broadcasting large task binary with size 1702.6 KiB\n",
      "25/05/23 23:04:43 WARN DAGScheduler: Broadcasting large task binary with size 1706.9 KiB\n",
      "25/05/23 23:04:43 WARN DAGScheduler: Broadcasting large task binary with size 1714.0 KiB\n",
      "25/05/23 23:04:44 WARN DAGScheduler: Broadcasting large task binary with size 1727.1 KiB\n",
      "25/05/23 23:04:44 WARN DAGScheduler: Broadcasting large task binary with size 1728.7 KiB\n",
      "25/05/23 23:04:44 WARN DAGScheduler: Broadcasting large task binary with size 1729.2 KiB\n",
      "25/05/23 23:04:45 WARN DAGScheduler: Broadcasting large task binary with size 1729.9 KiB\n",
      "25/05/23 23:04:45 WARN DAGScheduler: Broadcasting large task binary with size 1730.9 KiB\n",
      "25/05/23 23:04:45 WARN DAGScheduler: Broadcasting large task binary with size 1733.2 KiB\n",
      "25/05/23 23:04:46 WARN DAGScheduler: Broadcasting large task binary with size 1737.2 KiB\n",
      "25/05/23 23:04:46 WARN DAGScheduler: Broadcasting large task binary with size 1744.3 KiB\n",
      "25/05/23 23:04:46 WARN DAGScheduler: Broadcasting large task binary with size 1758.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions from Gradient Boosted Trees:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 23:04:47 WARN DAGScheduler: Broadcasting large task binary with size 1727.6 KiB\n",
      "25/05/23 23:04:47 WARN DAGScheduler: Broadcasting large task binary with size 1747.0 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|high_tip|prediction|\n",
      "+--------+----------+\n",
      "|       1|       1.0|\n",
      "|       0|       1.0|\n",
      "|       1|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 23:04:49 WARN DAGScheduler: Broadcasting large task binary with size 1735.8 KiB\n",
      "25/05/23 23:04:50 WARN DAGScheduler: Broadcasting large task binary with size 1747.0 KiB\n",
      "[Stage 1922:>                                                      (0 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees - Area under ROC: 0.8564\n",
      "Gradient Boosted Trees - Accuracy: 0.8729\n",
      "Gradient Boosted Trees - F1 Score: 0.8729\n",
      "Feature Importances:\n",
      "passenger_count: 0.0025\n",
      "trip_distance: 0.0076\n",
      "trip_duration_mins: 0.0071\n",
      "fare_amount: 0.0382\n",
      "pickup_hour: 0.0127\n",
      "pickup_day_of_week: 0.0046\n",
      "fare_per_mile: 0.0084\n",
      "fare_per_minute: 0.0070\n",
      "pickup_borough_vec: 0.0016\n",
      "dropoff_borough_vec: 0.0027\n",
      "payment_type: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create and train the Gradient Boosted Trees model\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_tip\",\n",
    "    maxIter=50,        # Number of iterations\n",
    "    maxDepth=8,        # Tree depth\n",
    "    stepSize=0.1,      # Learning rate\n",
    "    minInstancesPerNode=10,\n",
    "    maxBins=64,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample predictions from Gradient Boosted Trees:\")\n",
    "gbt_predictions.select(\"high_tip\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluate the model\n",
    "gbt_accuracy = multiclass_evaluator.evaluate(gbt_predictions)\n",
    "gbt_auc = binary_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "gbt_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_tip\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ").evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"Gradient Boosted Trees - Area under ROC: {gbt_auc:.4f}\")\n",
    "print(f\"Gradient Boosted Trees - Accuracy: {gbt_accuracy:.4f}\")\n",
    "print(f\"Gradient Boosted Trees - F1 Score: {gbt_f1:.4f}\")\n",
    "\n",
    "# Display feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_cols, gbt_model.featureImportances.toArray()):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b09386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "------------------------------------------------------------\n",
      "Model                     AUC        Accuracy   F1 Score  \n",
      "------------------------------------------------------------\n",
      "Logistic Regression       0.8547     0.8761     0.8716    \n",
      "Decision Tree             0.8556     0.8722     0.8722    \n",
      "Random Forest             0.8556     0.8723     0.8723    \n",
      "Gradient Boosted Trees    0.8564     0.8729     0.8729    \n",
      "\n",
      "Best model based on AUC: Gradient Boosted Trees with AUC = 0.8564\n",
      "Best model based on F1 Score: Gradient Boosted Trees with F1 = 0.8729\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "fare_amount: 0.0382\n",
      "pickup_hour: 0.0127\n",
      "fare_per_mile: 0.0084\n",
      "trip_distance: 0.0076\n",
      "trip_duration_mins: 0.0071\n"
     ]
    }
   ],
   "source": [
    "# Compare model performances\n",
    "model_performance = {\n",
    "    \"Logistic Regression\": {\"AUC\": lr_auc, \"Accuracy\": lr_accuracy, \"F1\": lr_f1},\n",
    "    \"Decision Tree\": {\"AUC\": dt_auc, \"Accuracy\": dt_accuracy, \"F1\": dt_f1},\n",
    "    \"Random Forest\": {\"AUC\": rf_auc, \"Accuracy\": rf_accuracy, \"F1\": rf_f1},\n",
    "    \"Gradient Boosted Trees\": {\"AUC\": gbt_auc, \"Accuracy\": gbt_accuracy, \"F1\": gbt_f1}\n",
    "}\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Model':<25} {'AUC':<10} {'Accuracy':<10} {'F1 Score':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for model, metrics in model_performance.items():\n",
    "    print(f\"{model:<25} {metrics['AUC']:<10.4f} {metrics['Accuracy']:<10.4f} {metrics['F1']:<10.4f}\")\n",
    "\n",
    "# Find the best model based on AUC\n",
    "best_model_auc = max(model_performance.items(), key=lambda x: x[1]['AUC'])\n",
    "print(f\"\\nBest model based on AUC: {best_model_auc[0]} with AUC = {best_model_auc[1]['AUC']:.4f}\")\n",
    "\n",
    "# Find the best model based on F1 score (better for imbalanced datasets)\n",
    "best_model_f1 = max(model_performance.items(), key=lambda x: x[1]['F1'])\n",
    "print(f\"Best model based on F1 Score: {best_model_f1[0]} with F1 = {best_model_f1[1]['F1']:.4f}\")\n",
    "\n",
    "# Analyze important features from the best model\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "if best_model_auc[0] == \"Random Forest\" or best_model_f1[0] == \"Random Forest\":\n",
    "    rf_importances = sorted(list(zip(feature_cols, rf_model.featureImportances.toArray())), key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in rf_importances[:5]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "elif best_model_auc[0] == \"Gradient Boosted Trees\" or best_model_f1[0] == \"Gradient Boosted Trees\":\n",
    "    gbt_importances = sorted(list(zip(feature_cols, gbt_model.featureImportances.toArray())), key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in gbt_importances[:5]:\n",
    "        print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841041ee",
   "metadata": {},
   "source": [
    "## Optimizing Prediction Threshold\n",
    "\n",
    "Prompt used: \"Review the notebook and propose areas for improvement. Optimize one of the 3 models implemented to increase its accuracy even further\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2d7b369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Gradient Boosted Trees doesn't support probability predictions.\n",
      "Using raw prediction scores for threshold optimization instead.\n",
      "\n",
      "Threshold optimization not applicable for Gradient Boosted Trees\n",
      "GBT uses internal threshold optimization during training.\n",
      "Current metrics for Gradient Boosted Trees:\n",
      "Accuracy: 0.8729\n",
      "F1 Score: 0.8729\n"
     ]
    }
   ],
   "source": [
    "# Let's optimize the threshold for models that support probability predictions\n",
    "# GBT doesn't support probability predictions, so we'll use a different approach\n",
    "best_model_name = best_model_auc[0]\n",
    "best_predictions = None\n",
    "\n",
    "if best_model_name == \"Logistic Regression\":\n",
    "    best_predictions = lr_predictions\n",
    "elif best_model_name == \"Random Forest\":\n",
    "    best_predictions = rf_predictions\n",
    "elif best_model_name == \"Decision Tree\":\n",
    "    best_predictions = dt_predictions\n",
    "elif best_model_name == \"Gradient Boosted Trees\":\n",
    "    print(f\"Note: {best_model_name} doesn't support probability predictions.\")\n",
    "    print(\"Using raw prediction scores for threshold optimization instead.\")\n",
    "    best_predictions = gbt_predictions\n",
    "\n",
    "if best_predictions is not None:\n",
    "    # Check if the model supports probability predictions\n",
    "    has_probability = \"probability\" in best_predictions.columns\n",
    "    \n",
    "    if has_probability and best_model_name != \"Gradient Boosted Trees\":\n",
    "        # For models with probability predictions (LR, RF, DT)\n",
    "        def evaluate_threshold(threshold):\n",
    "            from pyspark.sql.functions import when, col\n",
    "            from pyspark.ml.linalg import VectorUDT\n",
    "            from pyspark.sql.functions import udf\n",
    "            from pyspark.sql.types import DoubleType\n",
    "            \n",
    "            # Extract probability of positive class using UDF\n",
    "            def extract_prob(probability_vector):\n",
    "                if probability_vector is not None:\n",
    "                    return float(probability_vector[1])  # probability of class 1\n",
    "                return 0.0\n",
    "            \n",
    "            extract_prob_udf = udf(extract_prob, DoubleType())\n",
    "            \n",
    "            # Create custom prediction using threshold\n",
    "            threshold_predictions = best_predictions.withColumn(\n",
    "                \"prob_positive\",\n",
    "                extract_prob_udf(col(\"probability\"))\n",
    "            ).withColumn(\n",
    "                \"custom_prediction\",\n",
    "                when(col(\"prob_positive\") > threshold, 1.0).otherwise(0.0)\n",
    "            )\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"high_tip\",\n",
    "                predictionCol=\"custom_prediction\",\n",
    "                metricName=\"accuracy\"\n",
    "            ).evaluate(threshold_predictions)\n",
    "            \n",
    "            f1 = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"high_tip\",\n",
    "                predictionCol=\"custom_prediction\",\n",
    "                metricName=\"f1\"\n",
    "            ).evaluate(threshold_predictions)\n",
    "            \n",
    "            return threshold, accuracy, f1\n",
    "            \n",
    "    else:\n",
    "        # For GBT - use raw prediction scores\n",
    "        def evaluate_threshold(threshold):\n",
    "            from pyspark.sql.functions import when, col\n",
    "            \n",
    "            # For GBT, we can't optimize threshold in the same way\n",
    "            # since it doesn't provide probability scores\n",
    "            # We'll return the original metrics\n",
    "            accuracy = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"high_tip\",\n",
    "                predictionCol=\"prediction\",\n",
    "                metricName=\"accuracy\"\n",
    "            ).evaluate(best_predictions)\n",
    "            \n",
    "            f1 = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"high_tip\",\n",
    "                predictionCol=\"prediction\",\n",
    "                metricName=\"f1\"\n",
    "            ).evaluate(best_predictions)\n",
    "            \n",
    "            return threshold, accuracy, f1\n",
    "    \n",
    "    # Test different thresholds\n",
    "    if best_model_name != \"Gradient Boosted Trees\":\n",
    "        thresholds = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "        print(f\"\\nOptimizing threshold for {best_model_name}...\")\n",
    "        threshold_metrics = [evaluate_threshold(t) for t in thresholds]\n",
    "        \n",
    "        # Print threshold evaluation results\n",
    "        print(\"\\nThreshold Optimization Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Threshold':<10} {'Accuracy':<10} {'F1 Score':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        for threshold, accuracy, f1 in threshold_metrics:\n",
    "            print(f\"{threshold:<10.2f} {accuracy:<10.4f} {f1:<10.4f}\")\n",
    "        \n",
    "        # Find best threshold\n",
    "        best_threshold = max(threshold_metrics, key=lambda x: x[2])[0]\n",
    "        print(f\"\\nBest threshold: {best_threshold} (optimized for F1 score)\")\n",
    "        \n",
    "        # Apply the best threshold for final evaluation\n",
    "        from pyspark.sql.functions import udf\n",
    "        from pyspark.sql.types import DoubleType\n",
    "        \n",
    "        def extract_prob(probability_vector):\n",
    "            if probability_vector is not None:\n",
    "                return float(probability_vector[1])\n",
    "            return 0.0\n",
    "        \n",
    "        extract_prob_udf = udf(extract_prob, DoubleType())\n",
    "        \n",
    "        optimized_predictions = best_predictions.withColumn(\n",
    "            \"prob_positive\",\n",
    "            extract_prob_udf(col(\"probability\"))\n",
    "        ).withColumn(\n",
    "            \"optimized_prediction\",\n",
    "            when(col(\"prob_positive\") > best_threshold, 1.0).otherwise(0.0)\n",
    "        )\n",
    "        \n",
    "        # Calculate final metrics with optimized threshold\n",
    "        opt_accuracy = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"high_tip\",\n",
    "            predictionCol=\"optimized_prediction\",\n",
    "            metricName=\"accuracy\"\n",
    "        ).evaluate(optimized_predictions)\n",
    "        \n",
    "        opt_f1 = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"high_tip\",\n",
    "            predictionCol=\"optimized_prediction\",\n",
    "            metricName=\"f1\"\n",
    "        ).evaluate(optimized_predictions)\n",
    "        \n",
    "        print(f\"\\nFinal optimized metrics for {best_model_name}:\")\n",
    "        print(f\"Accuracy: {opt_accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {opt_f1:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # For GBT, show that threshold optimization isn't applicable\n",
    "        print(f\"\\nThreshold optimization not applicable for {best_model_name}\")\n",
    "        print(\"GBT uses internal threshold optimization during training.\")\n",
    "        print(f\"Current metrics for {best_model_name}:\")\n",
    "        print(f\"Accuracy: {gbt_accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {gbt_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
